% Use the lineno option to display guide line numbers if required.
\documentclass[9pt,a4paper]{amsart}

\usepackage{amsaddr,amsmath,caption,changepage,enumitem,graphicx,listings,booktabs,url}

% Use superscript numerical citations ala Nature
\usepackage[super,comma,compress]{natbib}

% Automatic formatting of SI units
\usepackage[binary-units,separate-uncertainty=true]{siunitx}

% Set options for code listings
\lstset{language=C++}

% Turn off whitespace around lists
%\setlist{nolistsep}

\newenvironment{fullwidth}{%
  \begin{adjustwidth}{-0.85in}{-0.85in}
}{\end{adjustwidth}}

% Visible TODO notes
\newcommand{\todo}[1]{\textbf{\textsc{\textcolor{red}{(TODO: #1)}}}}

\title{Larger GPU-accelerated brain simulations with procedural connectivity}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author{James C Knight and Thomas Nowotny}
\address{Centre for Computational Neuroscience and Robotics, School of Engineering and Informatics, University of Sussex, Brighton, United Kingdom} 
\email{J.C.Knight@sussex.ac.uk}

\begin{document}
\begin{abstract}
Simulations are an important tool for investigating brain function but large models are needed to faithfully reproduce the statistics and dynamics of brain activity.
Simulating large spiking neural network models has, until now, needed so much memory for storing synaptic connections that it required high performance computer systems.
Here, we present an alternative simulation method we call `procedural connectivity' where connectivity and synaptic weights are generated `on the fly' instead of stored and retrieved from memory.
This method is particularly well-suited for use on Graphical Processing Units~(GPUs)-- which are a common fixture in many workstations.
Extending our GeNN software with procedural connectivity and a second technical innovation for GPU code generation, we can simulate a recent model of the Macaque visual cortex with \num{4.13E6} neurons and \num{24.2E9} synapses on a single GPU -- a significant step forward in making large-scale brain modelling accessible to more researchers.
\end{abstract}
\maketitle
\section{Introduction}
The brain of a mouse has around \num{70E6} neurons, but this number is dwarfed by the \num{1E12} synapses which connect them~\citep{Herculano-Houzel2010}.
%While simulating synaptic plasticity -- the family of mechanisms believed to be %responsible for learning -- represents a further challenge, in a large-scale model, it is unlikely that learning would be enabled on \emph{all} synapses so efficiently simulating the remaining static synapses is a key challenge for large-scale brain simulation.\todo{has this been at all quantified in mice?}
In computer simulations of spiking neural networks, propagating spikes involves  applying the weighted synaptic input from each spiking presynaptic neuron to the postsynaptic neurons.
%Because typical EPSP shaping functions are linear, they can then be subsequently applied to the `histogram' resulting from this process.~\todo{presumably someone first had this intuition so cite}.
The information describing which neurons are synaptically connected and with what weight is typically generated before a simulation is run and stored in large arrays. 
For large-scale brain models this creates high memory requirements, so that they can typically only be simulated on large distributed computer systems using software such as NEST~\citep{Gewaltig2007} or NEURON~\citep{carnevale2006neuron}.
By careful design, these simulators can keep the memory requirements for each node almost constant, even when a simulation is distributed across tens of thousands of nodes~\citep{Jordan2018}.
However, high performance computer~(HPC) systems are bulky, expensive and consume a lot of power so are typically shared resources, only accessible to a limited number of researchers for time-limited investigations.

Neuromorphic systems~\citep{Frenkel2018,Furber2014,Merolla2014,Qiao2015,Schemmel2017} take inspiration from the brain and have been developed specifically for simulating large spiking neural networks more efficiently.
One particular relevant feature of the brain is that its memory elements -- the synapses -- are co-located with the computing elements -- the neurons.
In neuromorphic systems, this often translates to dedicating a large proportion of each chip to memory.
However, while such on-chip memory is fast, it can only be fabricated at relatively low density so that many of these systems economize -- either by reducing the maximum number of synapses per neuron to as few as \num{256} or by reducing the precision of the synaptic weights to \SI{6}{\bit}~\citep{Schemmel2017}, \SI{4}{\bit}~\citep{Frenkel2018} or even \SI{1}{\bit}~\citep{Merolla2014}.
This allows some classes of spiking neural networks to be simulated very efficiently but, reducing the degree of connectivity to fit within the constraints of current neuromorphic systems inevitably changes the dynamics of brain simulations~\citep{VanAlbada2015}.
Unlike most other neuromorphic systems, the SpiNNaker~\citep{Furber2014} neuromorphic supercomputer is fully programmable and combines large on-chip and external memories, distributed across the system, which enables real-time simulation of large-scale models~\citep{Rhodes2019}.
This is promising for the future but, due to its prototype nature, the availability of SpiNNaker hardware is limited and even moderately-sized simulations still require a physically large system (9 boards for a model with around \num{100E3} neurons and \num{300E6} synapses~\citep{Rhodes2019}).
%However, a physically large system is required for even moderately-sized simulations (9 boards for a simulation with around \num{100E3} neurons and \num{300E6} synapses~\citep{Rhodes2019})
%A next generation SpiNNaker system is currently under development~\citep{Mayr2019} and, by employing a newer fabrication technology (\SI{22}{\nano\meter} rather than \SI{130}{\nano\meter}), a single chip of the new system will offer equivalent performance to 48 of the current chips.
%Nonetheless, large-scale brain simulations will still require a large multi-chip system.

Modern GPUs have relatively little on-chip memory and, instead, dedicate the majority of their silicon area to arithmetic logic units.
GPUs use dedicated hardware to rapidly switch between tasks so that the latency of accessing external memory can be `hidden' behind computation, as long as there is sufficient computation to be performed.
For example, the memory latency of a typical modern GPU can be completely hidden if each CUDA core performs approximately 10 arithmetic operations per byte of data accessed from memory.
Unfortunately, propagating a spike in a spiking neural network simulation typically requires reading a synaptic weight ($\ge \SI{2}{\byte}$) and postsynaptic index ($\ge \SI{2}{\byte}$) from memory and then writing back the synaptic input ($\ge \SI{4}{\byte}$), while performing many fewer than the corresponding 80 instructions. 
This makes spike propagation highly memory bound.
Nonetheless, we have shown in previous work~\citep{Knight2018} that, as GPUs have significantly higher total memory bandwidth than even the fastest CPU, moderately sized models of around \num{100E3} neurons and \num{1E9} synapses can be simulated on a single GPU with competitive speed and energy consumption.
However, individual GPUs do not have enough memory to simulate larger brain models.
One can install a small number of GPUs within a single node and connect them using high-speed NVLink interconnect, through which they can communicate with low latency and high bandwidth~\citep{Li2020}.
However, if a larger number of GPUs is required, multiple nodes need to be used which are connected via the same MPI based protocols and network fabric as CPU based systems and hence suffer from similar communication overheads as CPU based clusters.

In this work, we present two innovations which enable large-scale brain simulations on a single GPU workstation.
The first innovation, which we describe in section~\ref{sec:results/procedural}, we call `procedural connectivity'.
Procedural connectivity uses the large amount of computational power available on a GPU to generate synaptic connectivity `on the fly' -- reducing memory and memory bandwidth requirements.
While a similar approach was used by Eugene Izhikevich for simulating an extremely large thalamo-cortical model with \num{1E11} neurons and \num{1E15} synapses on a modest PC cluster in 2005~\citep{Izhikevich2005} -- an incredible achievement -- it has not been subsequently applied to modern hardware.
Procedural connectivity vastly reduces the memory requirements of large-scale brain simulations but previous GPU code generation strategies also do not scale well to models containing large numbers of neural populations and synaptic projections.
To address this second problem, we have developed a `kernel merging' code generator, described in section~\ref{sec:results/kernel_merging}.
In section~\ref{sec:results/multi_area_model} we then demonstrate the power of these new features by simulating a recent model of the Macaque visual cortex~\citep{Schmidt2018} with \num{4.13E6} neurons and \num{24.2E9} synapses.
\vfill
\begin{figure}
    \begin{fullwidth}
        \centering
        \includegraphics{figures/performance_scaling}
        \captionsetup{width=6.69in}
        \caption{Scaling of \SI{1}{\second} balanced random network simulation using different algorithms on a range of modern GPUs. 
        All data points represent the mean of 5 simulations and standard deviations are smaller than line width so not shown.}
        \label{fig:performance_scaling}
    \end{fullwidth}
\end{figure}

\section{Results}
\subsection{Procedural connectivity}
\label{sec:results/procedural}
In a brain simulation, neurons and synapses can be described by a variety of mathematical models but these are eventually all translated into time or event-driven update algorithms~\citep{Brette2007}.
Our GeNN simulator~\citep{Yavuz2016} uses code generation to convert neuron and synapse update algorithms -- described using `snippets' of C-like code -- into CUDA code for efficient GPU simulation.
Before a simulation can be run, its parameters, in particular the state variables and the synaptic connectivity, need to be initialised.
Traditionally, this is done by running initialisation algorithms on the main CPU prior to the simulation.
The results are stored in CPU memory, uploaded to GPU memory and then used during the simulation.
We have recently extended GeNN to use code generation from code snippets to also generate efficient, parallel code for model initialisation~\citep{Knight2018}.
Offloading initialisation to the GPU in this way made it around $20\times$ faster on a desktop PC~\citep{Knight2018}, demonstrating that initialisation algorithms are well-suited for GPU acceleration.
Here, we are going one step further.
We realised that, if each synaptic connection can be re-initialised in less than the 80 operations required to hide the latency incurred when fetching its \SI{8}{\byte} of state from memory, it could be faster and vastly more memory efficient to regenerate synaptic connections on demand rather than storing them in memory.
This is the concept of procedural connectivity.
It is applicable whenever synapses are static -- plastic synapses which change their weights during a simulation will have to be simulated in the traditional way.

We implemented procedural connectivity in GeNN by repurposing our previously developed parallel initialisation methods.
Instead of running them once for all synapses at the beginning of the simulation, we rerun the methods during the simulation to regenerate the outgoing synapses of each neuron that fires a spike and immediately use the generated connections and weights to run the post-synaptic code which calculates the effect of the spike onto other neurons.
This is possible because the outgoing synaptic connections from each neuron are typically largely independent from those of other neurons as demonstrated by the two example connectivity generation algorithms described in methods section~\ref{sec:methods/procedural_connectivity}.

To verify our procedural connectivity algorithm and its implementation, and for an initial demonstration of the performance and scalability of procedural connectivity, we simulated a balanced random network of Leaky Integrate-and-Fire~(LIF) neurons (see section~\ref{sec:methods/va_benchmark} for model description) at scales ranging from \num{1E3} to \num{1E6} neurons (\num{100E3} to \num{100E9} synapses respectively) on a representative selection of NVIDIA GPU hardware:
Jetson TX2, a low-power embedded system with \SI{8}{\giga\byte} (shared memory);
Geforce MX130, a laptop GPU with \SI{2}{\giga\byte};
Geforce GTX 1650, a low-end desktop GPU with \SI{4}{\giga\byte};
and Titan RTX, a high-end workstation GPU with \SI{24}{\giga\byte}.
We compare the results and performance of the procedural connectivity based simulations to the standard approach of storing synaptic connections in memory employing two different data structures.
Both data structures are described in more detail in our previous work~\citep{Knight2018} but briefly, in the `sparse' data structure, a presynaptic neuron's postsynaptic targets are represented as an array of indices whereas, in the `bitfield' data structure, they are represented as a $N_{\text{post}}$ array of bits where a `1' at position $i$ indicates that there is a connection to postsynaptic neuron $i$ and a `0' that there is not.

For verification of the procedural connectivity implementation we recorded membrane voltages from two instantiations of the model with the same random seeds but one with procedural and one with sparse connectivity.
We found that, to \SI{32}{\bit} floating point precision, the membrane voltages of all neurons were identical between the two runs.

To measure performance, Fig.~\ref{fig:performance_scaling} shows the duration of all simulations using our new procedural approach and the standard approach with sparse and bitfield connectivities. 
None of our devices have enough memory to store the \num{100E9} synapses required for the largest scale using either data structure but, at the \num{100E3} neuron scale, the bitfield data structure allows the model to fit into the memory of several devices it otherwise would not.
However, not only is the new procedural approach the \emph{only} way of simulating models at the largest scales but, as Fig.~\ref{fig:performance_scaling} illustrates, even at smaller scales the performance of the procedural approach is competitive with and sometimes better than the standard approach.
Having removed the bottleneck of reading synaptic weights and postsynaptic indices from memory, one might have expected even higher performance. 
However, our current implementation still accumulates postsynaptic input using a `fire-and-forget' atomic add operation.
Even though atomic operations are entirely handled within the L2 cache of modern GPUs and thus have relatively low latency, profiling using NVIDIA Nsight compute suggests that the latency of these operations, combined with those incurred when sampling from the PRNG and calculating logarithms are currently the limiting factor in this benchmark rather than the compute resources of the GPU. This suggests that additional optimisations might allow for further significant performance improvements in the future.
All of the synapses in this model have the same synaptic weight meaning that they can be hard-coded into the procedural connectivity kernels.
If weights vary across synapses, the `bitfield' cannot be used and the memory constraints for the `sparse' representation become even more severe.

\begin{figure}
    \centering
    \includegraphics{figures/merging_scaling}
    \caption{Performance of a \SI{1}{\second} simulation of \num{1E6} LIF neurons driven by a Gaussian input current, partitioned into varying numbers~($N_{pop}$) of populations and running on a workstation equipped with a Titan RTX GPU.
    All data points represent the mean of 5 simulations and standard deviations are smaller than line width so not shown.
    \textbf{A} Compilation time~($T_{comp}$) using GCC 7.5.0.
    \textbf{B} Neuron kernel time~($T_{neuron}$) for a \SI{1}{\second} simulation.
    \textbf{C} Memory throughput~($K_{mem}$) reported by NVIDIA Nsight compute profiler ``Speed of light'' metric.
    \textbf{D} Number of ``No instruction'' stalls reported by NVIDIA Nsight compute profiler~($N_{stall}$).}
    \label{fig:merging_scaling}
\end{figure}

\subsection{Kernel merging}
\label{sec:results/kernel_merging}
NVIDIA GPUs are typically programmed in CUDA using a Single Instruction Multiple Thread~(SIMT) paradigm where programmers write `kernel' functions containing serial C-like code which is then executed in parallel across many virtual threads.
We call our second innovation `kernel merging' and it relates to the way these kernels are implemented.
While the procedural connectivity presented in the previous section allows models which would not otherwise fit into the memory of a GPU to be simulated, there are additional problems when using code generation for models with a large number of neuron and synapse populations.
GeNN and other SNN simulators which use code generation to generate all of their simulation code~\citep{Blundell2018} (as opposed to, for example NESTML~\citep{Plotnikov2016}, which uses code generation only to generate neuron simulation code) generate separate pieces of code for each population of neurons and synapses.
This allows optimizations such as hard-coding constant parameters and, although generating code for models with many populations will result in large code size, C++ CPU code can easily be divided between multiple modules and compiled in parallel, minimizing the effects on build time.
However, GPUs can only run a small number of kernels -- which are equivalent to modules in this context --  simultaneously (128 on the latest NVIDIA GPUs).
Therefore, GeNN employs a ``Kernel Fusion'' approach~\citep{Wang2010} where multiple neuron populations are simulated within each kernel.
CUDA threads are grouped into `thread blocks' and to maximise performance, the code running on  threads within a thread block should not `diverge'.
To minimise this divergence, we add `padding' threads around each population of neurons so population and block boundaries are aligned.
For example, if simulating 3 populations of \num{100} neurons using a thread block size of 32, each population would be padded to \num{128} threads. 
The update code for the first neuron population would then be conditioned on \lstinline{thread < 128} and the second on \lstinline{thread >= 128 && thread < 256} etc.

This approach works well for a small number of populations but, as Fig.~\ref{fig:merging_scaling}A illustrates, when we partition a model consisting of a single population of \num{1000000} LIF neurons (see section~\ref{sec:methods/merging} for model description) into an increasingly large number of ever smaller populations, compilation time increases super-linearly and quickly becomes impractical.
Furthermore, the simulation also runs slower with a large number of populations (Fig.~\ref{fig:merging_scaling}B).

Normally, we would expect this model to be memory bound as each thread in the model reads \SI{32}{\byte} of data (\SI{8}{\byte} of neuron state and \SI{24}{\byte} of RNG state) and, as discussed above, hiding the latency of these memory accesses would require approximately 320 arithmetic operations -- many more than required to sample an input current from the normal distribution and update a LIF neuron.
Fig.~\ref{fig:merging_scaling}C -- obtained using data from the NVIDIA Nsight compute profiler -- shows that this is true for small numbers of populations.
In this case, the memory system is around \SI{90}{\percent} utilised.
However, when the model is partitioned into larger numbers of smaller populations, the memory is used less efficiently and the kernel becomes latency bound, i.e.~neither memory \emph{nor} compute are used efficiently.
Investigating further, we found that this drop in performance was accompanied by an increasing number of ``No instruction'' stalls~(Fig.~\ref{fig:merging_scaling}D) which are events that prevent the GPU from doing any work during a clock cycle.
These particular events are likely to be caused by ``Excessively jumping across large blocks of assembly code'', which makes sense as we are generating kernels with hundreds of thousands of lines of code.
%\todo{is more detail required here as to why?}

Several neural modelling tools including Brian2~\citep{Stimberg2019} provide modellers with tools to work with `slices' of neuron populations, allowing models to be defined with fewer populations.
However, if a model is defined by connecting these slices together, the resulting connectivity is the result of \emph{multiple} simple connection rules of the type discussed in the previous section, making it much more difficult to apply our procedural connectivity approach.
Furthermore, such an approach places the responsibility for structuring a model in such a way that it can be simulated efficiently onto the modellers, who often prefer to concentrate on the science and organise populations according to anatomy or physiology.

To address the issue of too many populations, we developed a new code generator for GeNN which employs a Single Program Multiple Data~(SPMD) approach and `merges' the model description, grouping together populations which can be simulated using the same generated code.
From this merged description, structures are generated to store the pointers to state variables and the values of parameters if they differ between merged populations.
%
%\begin{lstlisting}
%struct NeuronUpdateGroup {
%  unsigned int numNeurons;
%  float* V;
%};
%\end{lstlisting}
%
An array of these structures is then declared for each merged population and each element is initialised with pointers to state variables and parameter values.
%
%\begin{lstlisting}
%NeuronUpdateGroup neuronUpdateGroup[3];
%neuronUpdateGroup[0] = {100, VA};
%neuronUpdateGroup[1] = {100, VB};
%neuronUpdateGroup[2] = {100, VC};
%\end{lstlisting}
%
%where \lstinline{VA} is a pointer to the array containing the state variable `V' of %populations `A' and so on.
In order for a thread to determine which neuron in which population it should simulate, we generate an additional array containing a cumulative sum of threads used for each population.
%\begin{lstlisting}
%unsigned int startThread[3] = {0, 128, 256};
%\end{lstlisting}
%
\begin{figure}
    \begin{fullwidth}
        \centering
        \includegraphics{figures/multi_area}
        \captionsetup{width=6.69in}
        \caption{Results of full-scale multi-area model simulation. 
        \textbf{A-C} Raster plots from a ground state simulation in GeNN, showing spiking activity of \SI{3}{\percent} of the neurons in areas V1~(A), V2~(B), and the Frontal Eye Field (FEF)~(C). Blue: excitatory neurons, red: inhibitory neurons.        
        \textbf{D-F} Spiking statistics across all 32 areas for each population for ground state simulations, using GeNN (orange) and NEST (blue), shown as split violin plots. Population-averaged firing rates (D), Average pairwise correlation coefficients of spiking activity (E) and Irregularity measured by revised local variation LvR~\citep{Shinomoto2009} averaged across neurons (F). Solid lines in violins: medians, dashed lines: interquartile range.
                \textbf{G-I} Raster plots as in (A-C) but for a resting state simulation in GeNN.
        \textbf{J-L} Same spiking statistics as (D-F) but for resting state simulations.}
        \label{fig:multi_area}
    \end{fullwidth}
\end{figure}
%
\begin{figure}
    \begin{fullwidth}
        \centering
        \includegraphics{figures/microcircuit_accuracy_kl}
        \captionsetup{width=6.69in}
        \caption{Comparison of full-scale multi-area model spike statistics between three GeNN simulations with different seeds; and between the three GeNN simulations and a NEST simulations. Comparisons calculated as the Kullback-Leibler (KL) divergence between the types of distributions displayed in Fig.~\ref{fig:multi_area}D-F,J-L. Bullets mark the mean and whiskers indicate the standard deviation of observed KL divergences across different pairs of compared distributions.
        \textbf{A,B} Neuron firing rates for ground~(A) and resting~(B) state.
        \textbf{C,D} Pairwise correlation coefficients of spiking activity in ground~(C) and resting~(D) state.
        \textbf{E,F} Irregularity measured by revised local variation LvR~\citep{Shinomoto2009} in ground~(E) and resting~(F) state.}
        \label{fig:microcircuit_accuracy_kl}
    \end{fullwidth}
\end{figure}
%
Each thread performs a simple binary search within this array to find the index of the neuron and population it should simulate.
As Fig.~\ref{fig:merging_scaling} shows, this approach solves the observed issues with compilation time and simulation performance. When kernel merging is used, all measures are essentially constant across the number of populations tested, with zero ``No instruction'' stalls. Interestingly, the additional overhead of a binary search does not affect the performance even for a large number of populations as the kernel execution is generally memory bound.

\subsection{The multi-area model}
\label{sec:results/multi_area_model}
Due to lack of computing power and sufficiently detailed connectivity data, previous models of the cortex have either focused on modelling individual local microcircuits at the level of individual cells~\citep{Izhikevich2008,Potjans2012} or modelling multiple connected areas at a higher level of abstraction~\citep{Cabral2014}.
However, recent data~\citep{Belitski2008} has shown that cortical activity has distinct features at both the global and local levels which can only be captured by modelling interconnected microcircuits at the level of individual cells.
The recent multi-area model~\citep{Schmidt2018a,Schmidt2018} is an example of such multi-scale modeling.
It uses scaled versions of a previous, 4 layer microcircuit model~\citep{Potjans2012} to implement \SI{1}{\milli\meter\squared} `patches' for 32 areas of the macaque visual cortex.
The patches are connected together according to inter-area axon tracing data from the CoCoMac~\citep{Bakker2012} database, further refined using additional anatomical data~\citep{Markov2014} and heuristics~\citep{Ercsey-Ravasz2013} to obtain estimates for the number of synapses between areas.
The synapses are distributed between populations in the source and target area using layer-specific tracing data~\citep{Markov2014b} and cell-type-specific dendritic densities~\citep{Binzegger2004} (see section~\ref{sec:methods/multi_area_model} for model description).
In 2018, this model was simulated using NEST~\citep{Gewaltig2007} on an IBM Blue Gene/Q supercomputer.
Because each Blue Gene/Q compute node only had a small amount of memory, these simulations were distributed across an entire rack (\num{1024} compute nodes) of the system and simulating \SI{1}{\second} of biological time took approximately \SI{12}{\minute}~\citep{Schmidt2018}.
On the newer `JURECA' supercomputer (where each node has \SI{128}{\giga\byte} of memory) this model can be simulated on as few as \num{11} compute nodes and,
when using \num{160} compute nodes, it can be initialized in approximately \SI{8}{\minute} and simulating \SI{1}{\second} of biological time takes only \SI{31}{\second}~\citep{VanAlbada2020}.

The model consists of \num{4.13E6} neurons in \num{254} populations and \num{24.2E9} synapses in \num{64516} projections.
Without kernel merging, it would therefore be unlikely that the model would compile or simulate at a workable speed using GeNN.
Furthermore, unlike the model we benchmarked previously, each synapse in this model has an independent weight and synaptic delay sampled from a normal distribution so that the bitfield data structure cannot be used; and
even if we assume that \SI{16}{\bit} floating-point would provide sufficient weight precision, that delays could be expressed as \SI{8}{\bit} integers and that neuron populations are all small enough to be indexed using \SI{16}{\bit} indices, our sparse data structure would still require \SI{5}{\byte} per synapse, such that the complete synaptic data would need over \SI{100}{\giga\byte} of GPU memory.
This is more than any single GPU has available.
However, using procedural connectivity, we are able to simulate this model on a workstation with a single Titan RTX GPU.

In order to test our implementation of the multi-area model with procedural connectivity and kernel merging in GeNN, we ran a \SI{10.5}{\second} simulation of the model in a `ground state' where inter-area connections have the same strength as intra-area connections and a \SI{100.5}{\second} simulation in a `resting state' where inter-area connections are $1.9\times$ stronger.
Initialisation of our model took \SI{6}{\minute} (\SI{3}{\minute} of which was spent generating and compiling code) and simulation of each biological second took \SI{7.9}{\minute} in the ground state and \SI{8.5}{\minute} in the resting state (averaged over 3 simulation runs).
While this is significantly faster than the older Blue Gene/Q supercomputer, it is around $15\times$ slower than the newer JURECA system.
However, because individual synapses simulated using procedural connectivity do not require initialisation, in shorter simulations this large performance difference can be somewhat offset by the faster initialisation time of the GeNN simulations.
For example a \SI{1}{\second} simulation would take just under \SI{11}{\minute} using GeNN (if the model is not recompiled) and around \SI{8.5}{\minute} using \num{160} JURECA nodes -- only \SI{2.5}{\minute} faster.

Figs.~\ref{fig:multi_area}A-C show some example spike rasters from three of the modelled areas, illustrating the asynchronous irregular nature of the model's ground state whereas Fig.~\ref{fig:multi_area}G-I illustrate the characteristic irregular activity and population bursts of the same areas in the resting state.
Next, we calculated the per-layer distributions of rates, spike-train irregularity and cross-correlation coefficients across all areas (disregarding the first \SI{500}{\milli\second} of simulation) and compared them to the same measures obtained from spike trains generated by the supercomputer simulations.
We calculated irregularity using the revised local variation LvR~\citep{Shinomoto2009}, averaged over a subsample of \num{2000} neurons and cross-correlation from spike histograms with \SI{1}{\milli\second} bins, calculated from a subset of \num{2000} non-silent neurons.
The distributions of these values -- obtained from the NEST and GeNN simulations -- are shown as violin plots in Fig.~\ref{fig:multi_area}D-F and Fig.~\ref{fig:multi_area}J-L.
Upon visual inspection, the distributions are very similar but, to assess how meaningful the remaining differences between these distributions are, we compared the distributions obtained from 3 GeNN simulations with different random number generator seeds to those obtained from the published NEST simulation.
We performed this comparison by computing histograms of the three measures (using bin sizes determined with the Freedman-Diaconis rule~\citep{Freedman1981}) and comparing pairs of distributions using the Kullback-Leibler divergence $D_\text{KL}$.
The results of these comparisons are shown in Fig.~\ref{fig:microcircuit_accuracy_kl} and suggest that the influence of the random seeds is comparable to the influence of random seeds and simulator combined -- indicating that the choice of simulator has little effect on the model dynamics.
%
\section{Discussion}
In this work we have presented a novel approach for large-scale brain simulation on GPU devices which entirely removes the need to store connectivity data in memory.
We have shown that this approach allows us to simulate a cortical model with \num{4.13E6} neurons and \num{24.2E9} synapses~\citep{Schmidt2018a,Schmidt2018} on a single modern GPU.
While this represents a significant step forward in terms of making large-scale brain modelling tools accessible to a large community of brain researchers, this model still has around $20\times$ fewer neurons and $40\times$ fewer synapses than the brain of even a small mammal such as a mouse~\citep{Herculano-Houzel2010}.
Our implementation of the multi-area model requires a little over \SI{12}{\giga\byte} of GPU memory, with the majority~(\SI{8.5}{\giga\byte}) being used for the circular dendritic delay buffers~(see~\citet{Knight2018}).
These are a per-neuron (rather than per-synapse) data structure but, because the inter-area connections in the model have delays of up to 500 simulation timesteps, the delay buffers become quite large.
However, for models without large delays such as the balanced random network simulated in section~\ref{sec:results/procedural}, only around \SI{20}{\byte} are requird per neuron meaning that, theoretically, over \num{1E9} neurons (over $10\times$ more than a mouse brain) could be simulated on a \SI{24}{\giga\byte} GPU.

One additional benefit of procedural connectivity not explored in this work is that parameters such as connection density or weight variance can be changed at runtime with minimal overhead.
%Structural plasticity has previously been used to tune the connectivity of large-scale brain model to achieve a desired level of activity~\citep{Diaz-Pier2016} and, while the procedural connectivity framework presented in this work would not allow full structural plasticity to be implemented, 
Furthermore, these modifications could be driven by the state of the network to implement a primitive form of structural plasticity.
% -- previously been used to tune the connectivity of large-scale brain model to achieve a desired level of activity~\citep{Diaz-Pier2016}.
%By modifying connectivity at runtime using a structural plasticity rule, \citet{Diaz-Pier2016} 
%Although the procedural connectivity framework presented in this work would not support the type of structural plasticity employed by \citet{Diaz-Pier2016} to tune the connectivity of large-scale brain model to achieve a desired level of activity, 
More generally, synaptic plasticity and learning are both important aspects of large-scale brain simulation not addressed in this work.
As discussed by~\citet{Knight2018}, GeNN supports a wide variety of synaptic plasticity rules.
In order to modify synaptic weights, they need to be stored in memory rather than generated procedurally.
However, connectivity could still be generated procedurally, potentially halving the memory requirements of models with synaptic plasticity.
This would be sufficient for synaptic plasticity rules where all updates can be performed at presynaptic spike times~\citep{Brader2007,Clopath2010c} but, for many STDP rules, updates triggered by postsynaptic spikes are also required.
GeNN supports such rules by automatically generating a lookup table structure (see~\citet{Knight2018}).
While this process could be adapted to generate a lookup table from procedural connectivity, this would further erode memory savings.
However, typically not all synapses in a simulation are plastic and those that are not could be simulated fully procedurally.

In this work, we have discussed the idea of procedural connectivity in the context of GPU hardware but, we believe that there is also potential for developing new types of neuromorphic hardware built from the ground up for procedural connectivity.
The latencies of sampling from the random number generator and performing atomic add operations were identified in section~\ref{sec:results/procedural} as the factors currently limiting performance of the procedural connectivity algorithm.
By dedicating more silicon area to fast on-chip memory -- which would alleviate the need for atomic operations -- and by implementing the random number generator in hardware, these limitations could be overcome, leading to potentially game-changing compute time improvements.

\section{Acknowledgments}
We would like to thank Jari Pronold, Sacha van Albada, Agnes Korcsak-Gorzo and Maximilian Schmidt for their help with the multi-area model data; and Dan Goodman and Mantas Mikaitis for their feedback on the manuscript.
JK and TN were funded by the EPSRC (Brains on Board project, grant number EP/P006094/1).
TN was also funded by the the European Unionâ€™s Horizon 2020 research and innovation program under Grant Agreements 785907 (HBP SGA2) and 945539 (HBP SGA3).

\section{Author contributions}
J.K. and T.N. wrote the paper.
T.N. is the original developer of GeNN.
J.K. is currently the primary GeNN developer and was responsible for extending the code generation approach to the procedural simulation of synaptic connectivity.
J.K. performed the experiments and the analysis of the results that are presented in this work.

\section{Methods}
\begin{table}
  \centering
  \begin{tabular}{r S S S}
    \toprule
        {Parameter}                             & {Procedural connectivity} & {Merging}         & {Multi-area} \\
                                                & {benchmark}               & {benchmark}       & {model} \\
    \midrule
        $\tau_{\text{m}}$ [\si{\milli\second}]         & 20                        & 20                & 10 \\
        $V_{\text{rest}}$ [\si{\milli\volt}]           & -60.0                     & -70.0             & -65 \\
       
        $V_{\text{th}}$ [\si{\milli\volt}]         & -50.0                     & -51.0             & -50 \\
        $R_{\text{m}}$ [\si{\mega\ohm}]                  & 20                        & 20                & 40 \\
        $\tau_{\text{ref}}$ [\si{\milli\second}]       & 5                         & 2                 & 2 \\
    \bottomrule
  \end{tabular}
  \caption{Neuron parameters.}
  \label{tab:parameters}
\end{table}
\subsection{Neuron model}
\label{sec:methods/neuron_model}
In all experiments presented in this work, neurons are modelled as Leaky Integrate-and-Fire~(LIF) units with the parameters listed in Table~\ref{tab:parameters}.
The membrane voltage $V_{i}$ of neuron $i$ is modelled as
%
\begin{align}
    \tau_{\text{m}} \frac{dV_{i}}{dt} = & (V_{\text{rest}} - V_{i}) + R_{\text{m}}(I_{\text{syn}_{i}} + I_{\text{ext}_{i}}), \label{eq:lif_neuron}
\end{align}
%
where $\tau_{\text{m}}$ and $R_{\text{m}}$ represent the time constant and resistance of the neuron's cell membrane, $V_{\text{rest}}$ defines the resting potential, $I_{\text{syn}_{i}}$ represents the synaptic input current and $I_{\text{ext}_i}$ represents an external input current.
When the membrane voltage crosses a threshold~$V_{\text{th}}$ a spike is emitted, the membrane voltage is reset to $V_{\text{rest}}$ and updating of $V$ is suspended for a refractory period $\tau_{\text{ref}}$.
In the models where there are synaptic connections, they are current-based, i.e.~pre-synaptic spikes lead to exponentially-decaying input currents $I_{\text{syn}_{i}}$
%
\begin{align}
    \tau_{\text{syn}} \frac{dI_{\text{syn}_{i}}}{dt} = & -I_{\text{syn}_{i}} + \sum_{i=0}^{n} w_{ij} \sum_{t_{j}}  \delta(t - t_{j}),\label{eq:exp_neuron_input_current}
\end{align}
%
where $\tau_{\text{syn}}$ represents the synaptic time constant and $t_{j}$ are the arrival times of incoming spikes from $n$ presynaptic neurons.
The ordinary differential equations~(\ref{eq:lif_neuron})~and~(\ref{eq:exp_neuron_input_current}) are solved with an exponential Euler algorithm.
%The continuous terms of the Eq.~\ref{eq:lif_neuron}~and~\ref{eq:exp_neuron_input_current} are separately solved algebraically so that the input currents $I_{\text{syn}_{i}}$ and $I_{\text{ext}_{i}}$ are treated as constants throughout each simulation timestep.

\subsection{Procedural connectivity algorithms}
\label{sec:methods/procedural_connectivity}
In the absence of knowledge of the exact microscopic connectivity in the brain, there are a number of typical connectivity schemes that are used in brain models.
In this section we discuss two typical examples and how they can be implemented efficiently on a GPU.
One very common connectivity scheme is the `fixed probability connector' for which each neuron in the presynaptic population is connected to each neuron in the postsynaptic population with fixed probability $P_{\text{conn}}$.
The postsynaptic targets of any presynaptic neuron can hence be sampled from a Bernoulli process with success probability $P_{\text{conn}}$.
One simple way of sampling from the Bernoulli process is to repeatedly draw samples from the uniform distribution $\text{Unif}[0, 1]$ and generate a synapse if the sample is less than $P_{\text{conn}}$.
However, for sparse connectivity ($P_{\text{conn}} \ll 1$), it is much more efficient to sample from the geometric distribution $\text{Geom}[P_{\text{conn}}]$ which governs the number of Bernoulli trials until the next success (i.e.~a synapse).
The geometric distribution can be sampled in constant time by inverting the cumulative density function of the equivalent continuous distribution (the exponential distribution) to obtain $\frac{log(\text{Unif}[0, 1])}{log(1 - P_{\text{conn}})}$~\citep{Devroye2013}.
Note that, if we were to directly draw from the uniform distribution, the sampling for each potential synapse would be independent from any other potential synapse and all these operations could be performed in parallel.
However, for the more efficient `geometric sampling' employed here, the sampling for the post-synaptic targets of a presynaptic neuron must be done serially, but is still independent from the sampling for any other presynaptic neuron.

Another common scheme for defining connectivity is the `fixed total number connector' in which a fixed total number $N_{\text{syn}}$ of synapses is placed between randomly chosen partners from the pre- and postsynaptic populations.
In order to initialise this connectivity in parallel, the number of synapses that originate from each of the $N_{\text{pre}}$ presynaptic neurons must first be calculated by sampling from the $N_{\text{pre}}$-category multinomial distribution $\text{Mult}[N_{\text{syn}}, \{\frac{1}{N_{\text{pre}}}, \frac{1}{N_{\text{pre}}}, \ldots, \frac{1}{N_{\text{pre}}}\}]$ up front on the host CPU because these numbers need to add to $N_{\text{syn}}$ and are hence not independent.
However, once the numbers of outgoing synapses are determined, the postsynaptic targets for a presynaptic neuron can be generated very efficiently in parallel by sampling from the discrete uniform distribution $\text{Unif}[0, N_{\text{post}}]$ where $N_{\text{post}}$ is the size of the postsynaptic population.
Note, that this can only be done because the targets of each presynaptic neuron are independent from those of any other presynaptic neuron.
Where synaptic weights and delays are not constant across synapses, but are described by some statistical distribution, they can also be sampled independently from each other and hence in parallel.
%
%Other typical connectivity schemes are described in the GeNN documentation\ref{}\todo{Uhm ... are they? Or should we gloss over this ...}
%'fixed number post' which is like fixed number total without the multinomial and 'fixed number pre' which we can't do easily :)

In order to use these parallel initialisation schemes for procedural connectivity, we require reproducible pseudorandom numbers that can be generated independently for each presynaptic neuron.
In principle this could be done with `conventional' pseudorandom  number generators~(PRNGs), but each presynaptic neuron would need to maintain its own PRNG state which would lead to a significant memory overhead.
Instead, we use the `counter-based' Philox4$\times$32-10 PRNG~\citep{Salmon2011}.
Counter-based PRNGs are designed for parallel applications and essentially consist of a pseudo-random bijective function which takes a counter as an input (for Philox4$\times$32-10 a \SI{128}{\bit} number) and outputs a random number.
In contrast to conventional PRNGs, this means that generating the $n^\text{th}$ random number in a stream has exactly the same cost as generating the `next' random number, allowing us to trivially divide up the random number stream between multiple parallel processes (in this case presynaptic neurons).
%\todo{do we need some more explanation of how you get from this to a network simulation?}

\subsection{Balanced random network model}
\label{sec:methods/va_benchmark}
This model was first presented by \citet{Vogels2005} but has subsequently been widely used as a scalable benchmark~\citep{Brette2007}.
The network consists of $N$ LIF neurons, modelled using the approach described in section~\ref{sec:methods/neuron_model} and configured with the parameters shown in table~\ref{tab:parameters}.
The neurons are partitioned into one population of $\frac{4N}{5}$ excitatory and a second of $\frac{N}{5}$ inhibitory neurons.
The two populations of neurons are connected to each other and with themselves with fixed probability $P_{\text{conn}}=\SI{10}{\percent}$ (the highest density at which \citet{Vogels2005} suggests their results hold).
All excitatory synapses have $\tau_{\text{syn}}=\SI{5}{\milli\second}$ and $w_{ij}=\frac{3.2}{N}\si{\nano\ampere}$; and all inhibitory synapses have $\tau_{\text{syn}}=\SI{10}{\milli\second}$ and $w_{ij}=\frac{40.8}{N}\si{\nano\ampere}$.
Additionally, every cell is depolarized by approximately \SI{10}{\milli\volt} by applying a constant external current $I_{\text{ext}_j}= \SI{0.55}{\nano\ampere}$.
Simulations were run with a time step $\Delta t = \SI{1}{\milli\second}$.

\subsection{Merging model}
\label{sec:methods/merging}
This model simply consists of \num{1000000} LIF neurons, again modelled using the approach described in section~\ref{sec:methods/neuron_model} and configured with the parameters shown in table~\ref{tab:parameters}. 
Each neuron is driven by an independent Gaussian input current of $I_{\text{ext}_j} \sim \mathcal{N}(1, 0.25^2)\SI{}{\nano\ampere}$.
Simulations were run with a time step $\Delta t = \SI{1}{\milli\second}$.

\subsection{Multi-area model}
\label{sec:methods/multi_area_model}
For a full description of the model, please refer to tables 2 and 3 in \citet{Schmidt2018}.
In our implementation, neurons are modelled using the approach described in section~\ref{sec:methods/neuron_model} and configured with the parameters shown in table~\ref{tab:parameters}.
Background Poisson input is delivered to each neuron via $I_{\text{ext}_{i}}$ with
%
\begin{align}
    \tau_{\text{syn}} \frac{dI_{\text{ext}_{i}}}{dt} = & -I_{\text{ext}_{i}} + J \text{Poisson}(\nu_{\text{ext}} \Delta t),
\end{align}
%
where $\tau_{\text{syn}}=\SI{0.5}{\milli\second}$; and the rate $\nu_{\text{ext}}$ and weight $J$ are calculated as described in tables 2 and 3 in \citet{Schmidt2018}.
Individual populations are connected by the fixed number connectors described in section~\ref{sec:results/procedural}.
Simulations were run with a time step $\Delta t = \SI{0.1}{\milli\second}$.

\section{Data availability}
The raw data used to produce Fig.~\ref{fig:performance_scaling} and Fig.~\ref{fig:merging_scaling}; and the pre-processed data used to produce Fig.~\ref{fig:multi_area} are available at \url{https://github.com/BrainsOnBoard/procedural_paper}.
The raw spiking data from the GeNN simulations of the multi-area model are available at \url{https://doi.org/10.25377/sussex.12912699}.
The raw spiking data from the NEST simulations run by \citet{Schmidt2018} are available from the original authors upon request.

\section{Code availability}
All experiments were carried out using the GeNN 4.3.3, available at \url{https://doi.org/10.5281/zenodo.4022384}.
A GeNN port of the multi-area model is available at \url{https://doi.org/10.5281/zenodo.4271816}.
The models used to produce Fig.~\ref{fig:performance_scaling} and Fig.~\ref{fig:merging_scaling} are all available at \url{https://github.com/BrainsOnBoard/procedural_paper}.

% Bibliography
\bibliographystyle{unsrtnat}
\bibliography{procedural}

\end{document}
