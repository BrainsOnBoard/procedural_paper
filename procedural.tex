% Use the lineno option to display guide line numbers if required.
\documentclass[9pt,twocolumn,twoside,lineno]{pnas-new}

\usepackage{enumitem,listings,booktabs}

% Automatic formatting of SI units
\usepackage[binary-units,separate-uncertainty=true]{siunitx}

% Set options for code listings
\lstset{language=C++}

% Turn off whitespace around lists
%\setlist{nolistsep}

% Visible TODO notes
\newcommand{\todo}[1]{\textbf{\textsc{\textcolor{red}{(TODO: #1)}}}}

\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\title{Large-scale brain simulations on the desktop using procedural connectivity}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,1]{James C Knight}
\author[a]{Thomas Nowotny} 

\affil[a]{Centre for Computational Neuroscience and Robotics, School of Engineering and Informatics, University of Sussex, Brighton, United Kingdom}

% Please give the surname of the lead author for the running footer
\leadauthor{Knight} 

% Please add here a significance statement to explain the relevance of your work
\significancestatement{Brain simulations are an important tool in helping to understand the function of the brain.
However, in order to reproduce some features found in recordings of biological brains, large models are required. 
Simulating such models has, until now, required such large amounts of memory that it can only be performed on large, expensive supercomputers.
In this work, we present a new method for simulating such models that massively reduces the amount of memory required.
This method is particularly well-suited to use on Graphics Processing Units~(GPUs) which are now a common fixture in many workstations.
Finally we demonstrate that we can simulate a very large brain model on a single GPU, \SI{35}{\percent} faster than previous supercomputer simulations.}

% Please include corresponding author, author contribution and author declaration information
\authorcontributions{J.K. and T.N. wrote the paper.
T.N. is the original developer of GeNN.
J.K. is currently the primary GeNN developer and was responsible for extending the code generation approach to the procedural simulation of synaptic connectivity.
J.K. performed the experiments and the analysis of the results that are presented in this work.}

\authordeclaration{The authors declare no conflict of interest.}
\correspondingauthor{\textsuperscript{1}To whom correspondence should be addressed. E-mail: J.C.Knight\@sussex.ac.uk}

% Keywords are not mandatory, but authors are strongly encouraged to provide them. If provided, please include two to five keywords, separated by the pipe symbol, e.g:
\keywords{spiking neural networks $|$ GPU $|$ high-performance computing $|$ brain simulation} 

\begin{abstract}
Large-scale simulations of spiking neural networks are important for improving our understanding of the dynamics and ultimately function of brains.
However, even small mammals such as mice have approximately \num{1E12} synaptic connections which are typically charaterized by at least one floating point value per synapse.
This amounts to several terabytes of connection data -- an unrealistic memory requirement for a single desktop machine.
Simulations of large spiking neural networks are therefore typically executed on large distributed supercomputers.
This is costly and limits large-scale modelling to a select few research groups with the appropriate resources.
%However, large parts of current brain models are described by simple algorithms which determine the existence and conductance of synaptic connections. 
In this work, we describe extensions to GeNN -- our GPU-based spiking neural network simulator -- that enable it to `procedurally' generate connectivity and synaptic weights `on the go' as spikes are triggered, instead of storing and retrieving them from memory.
We find that GPUs are well-suited to this approach because of their raw computational power, which due to memory bandwidth limitations is often under-utilised when simulating spiking neural networks.
We demonstrate the value of our approach with a recent model of the Macaque visual cortex consisting of \num{4.13E6} neurons and \num{24.2E9} synapses.
Using our new method, this model can be simulated on a single GPU.
Our results match those obtained on a supercomputer and the simulation runs \SI{35}{\percent} faster on a single high-end GPU than a previous simulation executed on over 1000 supercomputer nodes.
\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

\dropcap{T}he brain of a mouse has around \num{70E6} neurons, but this number is dwarfed by the \num{1E12}~\citep{Herculano-Houzel2010} synapses which connect them.
%While simulating synaptic plasticity -- the family of mechanisms believed to be %responsible for learning -- represents a further challenge, in a large-scale model, it is unlikely that learning would be enabled on \emph{all} synapses so efficiently simulating the remaining static synapses is a key challenge for large-scale brain simulation.\todo{has this been at all quantified in mice?}
In computer simulations of spiking neural networks, propagating spikes through synapses involves reading a `row' of synapses connecting a spiking presynaptic neuron to its postsynaptic partners and adding the `weight' of each synapse in the row to a `bin' containing the postsynatic neuron's input for the next simulation timestep.
%Because typical EPSP shaping functions are linear, they can then be subsequently applied to the `histogram' resulting from this process.~\todo{presumably someone first had this intuition so cite}.
Typically, the information describing which neurons are connected by a synapse and with what conductance, is generated before a simulation is run and stored in large matrices in random access memory~(RAM). 
This creates high memory requirements for large-scale brain models, so that they can typically only be simulated on large distributed computer systems using software such as NEST~\citep{Gewaltig2007} or NEURON~\citep{carnevale2006neuron}.
By careful design, these simulators can keep the memory requirements for each node constant, even when a simulation is distributed across thousands of nodes~\citep{Jordan2018}.
However, high performance computer systems are bulky, expensive and consume large amounts of power, meaning that they are typically shared resources that are only accessible to a limited number of researchers and for strongly time-limited investigations.

Neuromorphic systems~\citep{Frenkel2018,Frenkel2019,Furber2014,Merolla2014,Qiao2015,Schemmel2017} take inspiration from the brain and have been developed specifically for simulating large spiking neural networks.
One particular relevant feature of the brain is that its memory elements -- the synapses -- are co-located with the computing elements -- the neurons -- throughout the entire system.
In neuromorphic systems, this often translates to dedicating a large proportion of each chip to memory.
However, while such on-chip memory is fast, it can only be fabricated at relatively low density meaning that many of these systems economize -- either by reducing the maximum number of synapses per neuron to as few as \num{256} or by reducing the precision of the synaptic weights to \num{6}~\citep{Schemmel2017}, \num{4}~\citep{Frenkel2018} or even \SI{1}{\bit}~\citep{Merolla2014,Frenkel2019}.
Such strategies allow some classes of spiking neural networks to be simulated very efficiently, but reducing the degree of connectivity in large-scale brain simulations to fit within the constraints of current neuromorphic systems inevitably changes their dynamics~\citep{VanAlbada2015}.
Unlike the majority of other neuromorphic systems, the SpiNNaker~\citep{Furber2014} neuromorphic super-computer is entirely programmable and combines a large amount of on-chip memory with external memories, distributed across the system for the storage of synaptic connectivity.
SpiNNaker's external memory bandwidth, on-chip memory capacity and the computational power of each core are all tailored to large-scale brain simulation meaning that the output bins of the synapse processing algorithm can fit in on-chip memory and there is enough external memory bandwidth to fetch synaptic rows fast enough for real-time simulation of large-scale models~\citep{Rhodes2019}.
This is a promising approach for future research but, because of its prototype nature, the availability of SpiNNaker hardware is limited and a physically large system is still required for even moderately-sized simulations (9 boards for a simulation with around \num{10E3} neurons and \num{300E6} synapses~\citep{Rhodes2019}).
%However, a physically large system is required for even moderately-sized simulations (9 boards for a simulation with around \num{10E3} neurons and \num{300E6} synapses~\citep{Rhodes2019})
%A next generation SpiNNaker system is currently under development~\citep{Mayr2019} and, by employing a newer fabrication technology (\SI{22}{\nano\meter} rather than \SI{130}{\nano\meter}), a single chip of the new system will offer equivalent performance to 48 of the current chips.
%Nonetheless, large-scale brain simulations will still require a large multi-chip system.

Modern GPUs have relatively small amounts of on-chip memory and, instead, dedicate the majority of their silicon area to arithmetic logic units~(ALUs).
GPUs use dedictated hardware to rapidly switch between tasks so that the latency of accessing external memory can be `hidden' behind computation, as long as there is sufficient computation to be performed.
For example, the memory latency of a typical modern GPU can be completely hidden if each CUDA core performs approximately 10 arithmetic operations per byte of data accessed from memory.
Unfortunately, processing a synapse in a spiking neural network simulation is likely to require accessing approximately \SI{8}{\byte} of memory and performing many fewer than the required 80 instructions. This makes synaptic updates highly memory bound.
Nonetheless, we have shown in previous work~\citep{Knight2018} that, as GPUs have significantly higher total memory bandwidth than even the most expensive CPU, moderately sized models of around \num{10E3} neurons and \num{1E9} synapses can be simulated on a single GPU with competitive speed and energy requirements.
However, individual GPUs do not have enough memory to simulate truly large-scale brain models and, although small numbers of GPUs can be connected together using the high-speed NVLink~\citep{NVIDIACorporation} interconnect, beyond such small GPU clusters, scaling will be dictated by the same communication overheads as for other MPI-based distributed systems.

In this work we present a novel approach which converts large-scale brain simulation from a problem which is memory-bound on a GPU to one where the large amount of computational power available on a GPU can be used to reduce both memory and memory bandwidth requirements and enable truly large-scale brain simulations on a single GPU workstation.

\begin{figure*}
    \centering
    \includegraphics{figures/performance_scaling}
    \caption{Simulation time performance scaling on a range of modern GPUs (colors). \textbf{A} The best performing approach at each scale on each GPU (indicated by the symbols). For the largest models, the procedural method is always best.
    \textbf{B} Raw performance of each approach on each GPU.
    Missing bars indicate insufficient memory to simulate.}
    \label{fig:performance_scaling}
\end{figure*}

\section*{Results}
In the following subsections, we first present two recent innovations in our GeNN simulator~\citep{Yavuz2016} which allow us to use it for simulating very large models on a single GPU.
We then demonstrate the power of these new features by simulating a recent model of the Macaque visual cortex~\citep{Schmidt2018} consisting of \num{4.13E6} neurons and \num{24.2E9} synapses on a single GPU.
We find that we not only obtain the same results as in a previous simulation on a high-performance supercomputer, but that our simulation also runs faster.

\subsection*{Procedural connectivity}
The first crucial innovation to enable large scale simulations on a GPU is what we call `procedural connectivity'.
In a brain simulation, neurons and synapses can be described by a variety of mathematical models but, eventually, are translated into time or event-driven update algorithms~\citep{Brette2007} that calculate their state from previous states, allowing us to simulate their behaviour over time.
Our GeNN simulator~\citep{Yavuz2016} uses code generation to convert neuron and synapse update algorithms -- described using `snippets' of C-like code -- into CUDA code for efficient GPU simulation.
Before a simulation can be run, its parameters -- in particular the state variables and the synaptic connectivity -- need to be initialised.
Traditionally, this is done by running initialisation algorithms -- often involving random number generation -- once prior to the simulation on the main CPU.
The results are stored in CPU and GPU memories and used throughout the simulation.
We have recently extended GeNN to also use code generation to generate efficient, parallel model initialisation methods that run on the GPU from initialisation code snippets ~\citep{Knight2018}.
Offloading initialisation to the GPU in this way sped up model initialisation by around $20\times$ on a desktop PC~\citep{Knight2018}, demonstrating that initialisation algorithms are well-suited to GPU acceleration.
Here, we are going one step further.
We realised that, if each synaptic connection can be re-initialised in less than the 80 operations required to hide the latency incurred by fetching its parameter values from memory, it could be faster and vastly more memory efficient to regenerate synaptic connections on demand rather than storing them in memory.
This is the concept of procedural connectivity.
Although a similar approach was used by Eugene Izhikevich for simulating an extremely large thalamo-cortical model with \num{1E11} neurons and \num{1E15} synapses on a modest PC cluster in 2005~\todo{cite} -- an incredible achievement -- it has not been applied to more modern hardware since.  

We implemented procedural connectivity as an option in GeNN by repurposing our previously developed parallel initialisation methods.
Instead of being run once for all synapses at the beginning of the simulation, when using procedural connectivity, the methods are rerun during the simulation for the outgoing synapses of each neuron that fires a spike.
The identified connections and weights are then used to run the post-synaptic code that calculates the effect of the spike onto other neurons.
This is possible because outgoing synaptic connections from each neuron are typically largely independent from those of other neurons as we shall see from typical examples below.

In the absence of knowledge of the exact microscopic connectivity in the brain, there are a number of typical connectivity schemes that are used in brain models.
We will now discuss two typical examples and how they can be implemented efficiently on a GPU.
One very common connectivity scheme is the `fixed probability connector' which is described by a fixed probability $P_{\text{conn}}$ that a neuron in the presynaptic population will be connected to a neuron in the postsynaptic population.
In this case, the postsynaptic targets of any presynaptic neuron can be sampled from a Bernoulli process with success probability $P_{\text{conn}}$.
One simple way of sampling from the Bernoulli process is to repeatedly draw samples from the uniform distribution $\text{Unif}[0, 1]$ and generate a synapse if the sample is less than $P_{\text{conn}}$.
However, it is much more efficient to sample from the geometric distribution $\text{Geom}[P_{\text{conn}}]$ which is the distribution of the number of Bernoulli trials required to get the next success (i.e. a synapse).
The geometric distribution can be sampled in constant time by inverting the cumulative density function~(CDF) of the equivalent continuous distribution (the exponential distribution) to obtain $\frac{log(\text{Unif}[0, 1])}{log(1 - P_{\text{conn}})}$~\citep[p499]{DevroyeLuc2013}.
Note, that when directly drawing from the uniform distribution, the sampling for each potential synapse is completely independent from any other potential synapse and all these operations could be performed in parallel.
However, for the more efficient `beta-sampling' employed here, the sampling for the post-synaptic targets of a presynaptic neuron must be done serially, but is still independent from the sampling for any other presynaptic neuron.

Another common connectivity scheme for defining connectivity is the fixed number connector.
In this scheme the synaptic connections between two neuronal populations are characterised by a fixed total number ($N_{\text{syn}}$) of randomly placed synapses.
In order to initialise this connectivity in parallel, the subset of the $N_{\text{syn}}$ synapses which connect each presynaptic neuron must be calculated by sampling from the multinomial distribution $\text{Mult}[N_{\text{syn}}, \{P_{row}, P_{row}, \ldots, P_{row}\}]$ where $P_{row} = \frac{N_{\text{post}}}{N_{\text{pre}} N_{\text{post}}} =  \frac{1}{N_{\text{pre}}} $, because the numbers for different presynaptic neurons are not independent from each other.
This operation cannot be efficiently parallelised so we perform it on the host CPU and store the results in a GPU memory array.
However, once the numbers of outgoing synapses are determined, the postsynaptic targets for a presynaptic neuron can be generated very efficiently in parallel by sampling from the discrete uniform distribution $\text{Unif}[0, N_{\text{post}}]$.
Note, that this can only be done because the targets of each presynaptic neuron are independent from those of any other pre-synaptic neuron.

Where synaptic weights and delays are not constant across synapses, but are described by some statistical distribution, they can also be sampled independently from each other and hence in parallel.
%
%Other typical connectivity schemes are described in the GeNN documentation\ref{}\todo{Uhm ... are they? Or should we gloss over this ...}
%'fixed number post' which is like fixed number total without the multinomial and 'fixed number pre' which we can't do easily :)

In order to use these parallel initialisation schemes for procedural connectivity, we require reproducible pseudorandom numbers that can be generated independently for each pre-synaptic neuron.
In principle this could be done with `convential' random number generators~(RNGs), but each presynaptic neuron would need to maintain its own RNG state which would lead to a significant memory overhead.
Instead, we use a `counter-based' Philox4$\times$32-10 RNG~\citep{Salmon2011}.
Counter-based RNGs are designed for parallel applications and essentially consist of a pseudo-random bijective function which takes a counter as an input (in this case a \SI{128}{\bit} number) and outputs random numbers.
In constrast to convential RNGs, this means that generating the $n^\text{th}$ random number in a stream has exactly the same cost as generating the `next' random number, allowing us to trivially divide up the random number stream between multiple parallel processes (in this case presynaptic neurons).
%\todo{do we need some more explanation of how you get from this to a network simulation?}

For an initial demonstration of the performance and scalability of procedural connectivity, we used a network that was initially designed to investigate signal propagation through cortical networks~\citep{Vogels2005}, but subsequently has been widely used as a scalable benchmark~\citep{Brette2007}.
The network consists of $N$ integrate-and-fire neurons, partitioned into $\frac{4N}{5}$ excitatory and $\frac{N}{5}$ inhibitory neurons.
The two populations of neurons are connected to each other and with themselves with a fixed $P_{\text{conn}}=\SI{10}{\percent}$ connection probability.

We ran simulations of this network at scales ranging from \num{1E3} to \num{1E6} neurons (\num{100E3} and \num{100E9} synapses respectively) on a representative selection of modern NVIDIA GPU hardware:
%
\begin{description}[noitemsep]
    \item [Jetson TX2] a low-power embedded system designed for robotic applications with \SI{8}{\giga\byte} of shared memory
    \item [Geforce MX130] a laptop GPU with \SI{2}{\giga\byte} of dedicated memory
    \item [Geforce GTX 1650] a low-end desktop GPU with \SI{4}{\giga\byte} of dedicated memory
    \item [Titan RTX] a high-end workstation GPU with \SI{24}{\giga\byte} of dedicated memory
\end{description}
%
In Fig.~\ref{fig:performance_scaling} we compare the duration of these simulations using our new procedural approach against the standard approach of storing synaptic connections in memory using two different data structures.
Both data structures are described in more detail in our previous work~\citep{Knight2018} but briefly, in the `sparse' data structure, a presynaptic neuron's postsynaptic targets are represented as an array of indices whereas, in the `bitfield' data structure, they are represented as a $N_{\text{pre}} \times N_{\text{post}}$ array of bits where a `1' at position $i$, $j$ indicates the existence of a synapse between neurons $i$ and $j$ and `0' its absence.
None of the devices used have enough memory to store the \num{100E9} synapses required for the largest scale using either data structure but, at the \num{100E3} neuron scale, the bitfield data structure allows the model to fit into the memory of several devices it otherwise would not.
However, not only is the new procedural approach the \emph{only} way of simulating models at the largest scales but, as Fig.~\ref{fig:performance_scaling} illustrates, even at smaller scales its performance is competitive with and sometime better than the standard approach.
All of the excitatory and inhibitory synapses in this model have the same synaptic weight meaning that they can be hard-coded into the procedural connectivity kernels.
However, if the weights vary across synapses, the `bitfield' representation cannot be used and the memory constraints for the 'sparse' representation become even more severe.

\begin{figure}
    \centering
    \includegraphics{figures/merging_scaling}
    \caption{Performance of a simulation of \num{1000000} LIF neurons driven by a gaussian input current, partitioned into varying numbers~($N_{pop}$) of populations and running on a workstation equipped with a Titan RTX GPU.
    \textbf{A} Compilation time~($T_{comp}$) using GCC 7.5.0.
    \textbf{B} Simulation time~($T_{sim}$) for an \SI{1}{\second} simulation.
    \textbf{C} Memory throughput~($K_{mem}$) reported by NVIDIA Nsight compute profiler 'Speed of light' metric.
    \textbf{D} Number of 'No instruction' stalls reported by NVIDIA Nsight compute profiler~($N_{stall}$).}
    \label{fig:merging_scaling}
\end{figure}

\subsection*{Kernel merging}
We call our second innovation ``kernel merging'' and it relates to the way code is organised into CUDA kernel functions.
While the procedural connectivity approach presented in the previous section allows us to simulate models which would otherwise not fit within the memory of a single GPU, there are additional problems when using code generation to generate simulation code for models with a large number of neuron and synapse populations.

GeNN and -- to the best of our knowledge~\citep{Blundell2018} -- all other SNN simulators which use code generation to generate all of their simulation code (as opposed to, for example NESTML~\citep{Plotnikov2016}, which uses code generation only to generate neuron simulation code) generate seperate pieces of code for each population of neurons and synapses.
This approach allows optimizations such as hard-coding constant parameters and, although generating code for models with many populations will result in large code size, C++ CPU code can easily be divided between multiple modules and compiled in parallel, minimizing the effects on build time.
However, GPUs can only run a small number of kernels -- which are equivalent to modules in this context --  simultaneously (128 on the latest NVIDIA GPUs~\citep[p278]{NVIDIACorporation2019}).
Therefore, in GeNN, multiple neuron populations are simulated within each kernel, resulting in code of the form shown in the following pseudocode which illustrates how 3 populations of 1000 neurons each could be simulated in a single kernel:
\todo{very minimal sentance about SIMT here}

\begin{lstlisting}
void updateNeurons()
{
  if(thread < 1000) {
    // Update neuron population A
  }
  else if(thread >= 1000 && thread < 2000) {
    // Update neuron population B
  }
  else if(thread >= 2000 && thread < 3000) {
    // Update neuron population C
  }
}

\end{lstlisting}
%
This approach works well for models with a small number of populations but, as Fig.~\ref{fig:merging_scaling}A illustrates, when we partition a model consisting of \num{1000000} LIF neurons into an increasingly large number of (smaller and smaller) populations, compilation time increases super-linearly as the size of the neuron kernel increases -- quickly becoming impractical.
Furthermore, as Fig.~\ref{fig:merging_scaling}B shows, the simulation also runs much more slowly when the model is partitioned into a large number of populations.
Normally, we would expect this model to be memory bound as each thread in the model reads \SI{32}{\byte} of data and, as we discussed previously, hiding the latency of these memory accesses would require approximately 320 arithmetic operations which is many more than are required to sample from the uniform distribution and update a LIF neuron.
Fig.~\ref{fig:merging_scaling}C -- obtained using data from the NVIDIA Nsight compute profiler~\citep{NVIDIACorporation2020} -- shows that this is true for small numbers of populations.
In this case, the memory system is around \SI{90}{\percent} utilised.
However, when the model is partitioned into a larger number of smaller populations, the memory is used less efficiently and the kernel becomes latency bound, i.e. neither memory \emph{nor} compute are used efficiently.
Investigating further using the profiler, we found that this drop in performance was accompanied by an increasing number of ``No instruction'' stalls as shown in Fig.~\ref{fig:merging_scaling}D.
Stalls are events which prevent the GPU from doing any work during a clock cycle and the profiler documentation suggests that these particular events are likely to be caused by ``Excessively jumping across large blocks of assembly code''~\citep[p47]{NVIDIACorporation2020} -- which makes sense when we are generating kernels with hundreds of thousands of lines of code.
%\todo{is more detail required here as to why?}

To address these issues, we developed a new code generator for GeNN which first `merges' the model description, grouping together populations which can be simulated using the same generated code.
From this merged description, structures are generated to store the pointers to state variables and parameter values which are still allowed to differ between merged populations:
%
\begin{lstlisting}
struct NeuronUpdateGroup
{
  unsigned int numNeurons;
  float* V;  
};
\end{lstlisting}
%
An array of these structures is then declared for each merged population and each element is initialised with pointers to state variables and parameter values:
%
\begin{lstlisting}
NeuronUpdateGroup neuronUpdateGroup[3];
neuronUpdateGroup[0] = {1000, VA};
neuronUpdateGroup[1] = {1000, VB};
neuronUpdateGroup[2] = {1000, VC};
\end{lstlisting}
%
where \lstinline{VA} is a pointer to the array containing the state variable `V' of populations `A' and so on.
In order for a thread to determine which neuron in which population it should simulate, we generate an additional data structure -- an array containing a cumulative sum of threads used for each population.
Each thread performs a simple binary search within this array to find the index of the neuron and population it should simulate:
%
\begin{lstlisting}
unsigned int startThread[3] = {0, 1000, 2000};
void updateNeurons()
{
  if(thread < 3000) {
    // Binary search in startThread to 
    // determine which neuron in which 
    // population should be processed. 
    // Then update, accessing variables 
    // through neuronUpdateGroup
  }
}
\end{lstlisting}
%
As Fig.~\ref{fig:merging_scaling} shows, this approach solves the issues with compilation time and simulation performance caused by large numbers of populations.

\begin{figure*}
    \centering
    \includegraphics{figures/multi_area}
    \caption{Results of full-scale multi-area model simulation. 
    \textbf{A-C} Raster plots of spiking activity of \SI{3}{\percent} of the neurons in area V1~\textbf{A}, V2~\textbf{B}, and FEF~\textbf{C}. 
    Blue: excitatory neurons, red: inhibitory neurons.
    \textbf{D-F} Spiking statistics for each population across all 32 areas simulated using GeNN and NEST shown as split violin plots.
    Solid lines: medians, Dashed lines: Interquartile range~(IQR).
    \textbf{D} Population-averaged firing rates.
    \textbf{E} Average pairwise correlation coefficients ofspiking activity. 
    \textbf{F} Irregularity measured by revised local variation LvR~\citep{Shinomoto2009} averaged across neurons.}
    \label{fig:multi_area}
\end{figure*}

\subsection*{The multi-area model}
Due to lack of computing power and sufficiently detailed connectivity data, previous models of the cortex have either focussed on modelling individual local microcircuits at the level of individual cells~\citep{Izhikevich2008,Potjans2012} or modelling multiple connected areas at a higher level of abstraction where entire ensembles of neurons are described by a small number of differential equations~\citep{Cabral2014}.
However, data from several species~\todo{find citation} has shown that cortical activity has distinct features at both the global and local levels which can only be captured by modelling interconnected microcircuits at the level of individual cells.
The recent multi-area model~\citep{Schmidt2018a,Schmidt2018} is an example of such multi-scale modeling -- using scaled versions of a previous, 4 layer microcircuit model~\citep{Potjans2012} to implement \SI{1}{\milli\meter\squared} `patches' for each of 32 areas of the macaque visual cortex.
The 32 areas are connected together with connectivity based on inter-area axon tracing data from the CoCoMac~\citep{Bakker2012} database, further refined using additional anatomical data~\citep{Markov2014} and heuristics~\citep{Ercsey-Ravasz2013} to obtain estimates for the number of synapses between areas.
These synapses are distributed between populations in the source and target area using layer-specific tracing data~\citep{Markov2014b} and cell-type-specific dendritic densities~\citep{Binzegger2004}.
Individual populations are connected by the fixed number connectors described previously.
For a full description of the construction of the multi-area model please refer to the original works~\citep{Schmidt2018a,Schmidt2018}.
In 2018, this model was simulated using NEST~\citep{Gewaltig2007} on one rack of an IBM Blue Gene/Q supercomputer~(a \SI{2}{\metre} high enclosure containing \num{1024} compute nodes and weighing over \SI{2}{\tonne}).
On this system, initialization of the model took around \SI{5}{\minute} and simulating \SI{1}{\second} of biological time took approximately \SI{12}{\minute}~\citep{Schmidt2018}.

The multi-area model consists of \num{4.13E6} neurons split into \num{254} populations and \num{24.2E9} synapses split into \num{64516} populations meaning that, without the kernel merging approach presented above the model would be unlikely to compile or simulate at a workable speed using GeNN.
Additionally, unlike the model we benchmarked previously, each synapse in this model has an independant weight and synaptic delay sampled from a normal distribution, meaning that the bitfield data structure cannot be used to represent the connectivity.
Even if we assume that \SI{16}{\bit} floating point would provide sufficient weight precision, that delays could be expressed as an \SI{8}{\bit} integer and that the neuron populations are all small enough to be indexed using \SI{16}{\bit} indices, our sparse data structure would still require \SI{5}{\byte} per synapse, meaning that this model's synaptic data would require over \SI{100}{\giga\byte} of GPU memory.
While a cluster of GPUs connected using NVLink could be built with this much memory, it is more than any single GPU has available.
However, with the procedeural connectivity method, we are able to simulate this model on single workstation with one Titan RTX GPU with \SI{24}{\giga\byte} of memory.

In order to validate our GeNN simulations of the multi-area model, we ran a \SI{10.5}{\second} simulation of the model.
Initialization of our model took \SI{6}{\minute} -- \SI{3}{\minute} of which was spent generating and compiling code -- and simulation of each biological second took \SI{7.7}{\minute} -- \SI{35}{\percent} less than in the previous supercomputer simulation.
Fig.~\ref{fig:multi_area}A-C shows some example spike rasters from three of the modelled areas, illustrating the asynchronous irregular nature of the model's ground state.
Next, we calculated the per-layer distributions of rates, spike-train irregularity and cross-correlation coefficients across all areas (disregarding the first \SI{500}{\milli\second} of simulation) and compared them to the previously published values of the same measures obtained from the supercomputer simulations.
We calculated irregularity using the revised local variation LvR~\citep{Shinomoto2009}, averaged over a subsample of \num{2000} neurons and cross-correlation from spike histograms with \SI{1}{\milli\second} bins, calculated from a subset of \num{2000} non-silent neurons.
The violin plots in Fig.~\ref{fig:multi_area}D-F shows the comparison of the distributions of values obtained from the two simulations -- which are essentially identical.

\section*{Discussion}
In this work we have presented a novel approach for large-scale brain simulation on GPU devices which entirely removes the need to store connectivity data in memory.
We have shown that this approach allows us to simulate a cortical model with \num{4.13E6} neurons and \num{24.2E9} synapses~\citep{Schmidt2018a,Schmidt2018} on a single modern GPU.
While this represents a significant step forward in terms of making truly large-scale brain modelling tools accesible to a large community of brain researchers, this model still has around $20\times$ fewer neurons and $40\times$ fewer synapses than the brain of even a small mammal such as a mouse~\citep{Herculano-Houzel2010}.
Our implementation of the multi-area model requires a little over \SI{12}{\giga\byte} of GPU memory in total, with the majority~(\SI{8.5}{\giga\byte}) being used for the implementation of the circular buffers used to simulate dendritic delay~(described in more detail in our previous work~\citep{Knight2018}).
These are a per-neuron (rather than per-synapse) data structure but, because the inter-area connections in the model have delays of up to around \SI{50}{\milli\second} (500 \SI{0.1}{\milli\second} timesteps), the delay buffers become extremely large.

One important aspect of large-scale brain simulations that we have not addressed in this work is synaptic plasticity and its role in learning.
As discussed in our previous work~\citep{Knight2018}, GeNN has support for a wide variety of synaptic plasticity rules.
In order to modify synaptic weights, they need to be stored in memory rather than generated procedurally.
However, connectivity could still be generated procedurally, potentially halving the memory requirements of models with synaptic plasticity.
This would be sufficient for many synaptic plasticity rules that only require access to presynaptic spikes and postsynaptic neuron states such as membrane voltage~\cite{Brader2007,Clopath2010c}, but for many Spike-Timing-Dependent Plasticity~(STDP) rules access to \emph{postsynaptic} spikes is also required.
GeNN supports such rules by automatically generating a suitable lookup table structure (see our previous work~\citep{Knight2018} for more details) and this process could be adapted to generate a lookup table from procedural connectivity although this would further erode memory savings.
However, typically not all synapses in a simulation are plastic and those that are not could be simulated fully procedurally.

In this work, we have discussed the idea of procedural connectivity purely in the context of GPU hardware but, we believe that there is also some potential for developing new types of neuromorphic hardware built from the ground up for procedural connectivity.
Key components such as the counting random number generator could be implemented directly in hardware leading to truly game-changing compute time improvements.

\matmethods{
\begin{table}
  \centering
  \caption{Model parameters. \label{tab:parameters}}
  \begin{tabular}{r S S S}
    \toprule
        {Parameter}                             & {Procedural connectivity} & {Merging}         & {Multi-area} \\
                                                & {benchmark}               & {benchmark}       & {model} \\
    \midrule
        $\tau_{\text{m}}$ [\si{\milli\second}]         & 20                        & 20                & 2 \\
        $V_{\text{rest}}$ [\si{\milli\volt}]           & -60.0                     & -70.0             & -65 \\
       
        $V_{\text{th}}$ [\si{\milli\volt}]         & -50.0                     & -51.0             & -50 \\
        $R_{\text{m}}$ [\si{\mega\ohm}]                  & 20                        & 20                & 40 \\
        $\tau_{\text{syn}}$ [\si{\milli\second}]       & {$5/10$\textsuperscript{1}} & {---}             & 0.5\\
        $\tau_{\text{ref}}$ [\si{\milli\second}]       & 5                         & 2                 & 2 \\
        $I_{\text{in}_j}$ [\si{\nano\ampere}]           & 0.55                      & 1.0 \pm 0.25   & {Poisson\textsuperscript{2}}\\
        $w_{ij}$ [\si{\nano\ampere}]            & {$\frac{3.2}{N}/\frac{40.8}{N}$\textsuperscript{1}}           & {---}            & {Various\textsuperscript{2}}\\
    \bottomrule
  \end{tabular}
  \parbox{\columnwidth}{\textsuperscript{1}Excitatory/Inhibitory. \textsuperscript{2}Please refer to original works\citep[Table 1,2]{Schmidt2018}}
\end{table}
In all three experiments presented in this work, neurons are modelled as leaky integrate-and-fire~(LIF) units using the parameters listed in Table~\ref{tab:parameters}.
The membrane voltage $V_{i}$ of neuron $i$ is modelled as
%
\begin{align}
    \tau_{\text{m}} \frac{dV_{i}}{dt} = & (V_{i} - V_{\text{rest}}) + R_{\text{m}} I_{\text{in}_{j}}, \label{eq:lif_neuron}
\end{align}
%
where $\tau_{\text{m}}$ and $R_{\text{m}}$ represent the time constant and resistance of the neuron's cell membrane, $V_{\text{rest}}$ defines the resting potential and $I_{\text{in}_{j}}$ represents the input current.
When the membrane voltage crosses a threshold~$V_{\text{th}}$ a spike is emitted, the membrane voltage is reset to $V_{\text{rest}}$ and membrane potential integration is suspended for a refractory period $\tau_{\text{ref}}$.
In both models pre-synaptic spikes lead to exponentially-decaying input currents $I_{\text{in}_{j}}$
%
\begin{align}
    \tau_{\text{syn}} \frac{dI_{\text{in}_{i}}}{dt} = & -I_{\text{in}_{i}} + \sum_{i=0}^{n} w_{ij} \sum_{t_{j}}  \delta(t - t_{j}),\label{eq:exp_neuron_input_current}
\end{align}
%
where $\tau_{\text{syn}}$ represents the decay time constant and $t_{j}$ are the arrival times of incoming spikes from $n$ presynaptic neurons.
In all models, the continuous terms of the Eq.~\ref{eq:lif_neuron}~and~\ref{eq:exp_neuron_input_current} are seperately solved algebraically so that the synaptic input current $I_{\text{in}_{i}}$ entering into equation~\ref{eq:lif_neuron} is treated as a constant during each simulation timestep.}
\showmatmethods{} % Display the Materials and Methods section

\acknow{
We like to thank Jari Pronold, Sacha van Albada and Maximilian Schmidt for their valuable assistance with using the data and analysis tools published alongside the multi-area model -- without these contributions, our comparison with these results would not have been possible.
This work was funded by the EPSRC (Brains on Board project, grant number EP/P006094/1).}
\showacknow{} % Display the acknowledgments section

% Bibliography
\bibliography{procedural}

\end{document}
