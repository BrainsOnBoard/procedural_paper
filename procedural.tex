% Use the lineno option to display guide line numbers if required.
\documentclass[9pt,a4paper]{amsart}

\usepackage{amsaddr,amsmath,caption,changepage,enumitem,graphicx,natbib,listings,booktabs,url}

% Automatic formatting of SI units
\usepackage[binary-units,separate-uncertainty=true]{siunitx}

% Set options for code listings
\lstset{language=C++}

% Turn off whitespace around lists
%\setlist{nolistsep}

\newenvironment{fullwidth}{%
  \begin{adjustwidth}{-0.85in}{-0.85in}
}{\end{adjustwidth}}

% Visible TODO notes
\newcommand{\todo}[1]{\textbf{\textsc{\textcolor{red}{(TODO: #1)}}}}

\title{Larger GPU-accelerated brain simulations with procedural connectivity}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author{James C Knight and Thomas Nowotny}
\address{Centre for Computational Neuroscience and Robotics, School of Engineering and Informatics, University of Sussex, Brighton, United Kingdom} 
\email{J.C.Knight@sussex.ac.uk}

\begin{document}
\begin{abstract}
Large-scale simulations of spiking neural network models are an important tool for improving our understanding of the dynamics and ultimately the function of brains.
However, even small mammals such as mice have on the order of \num{1E12} synaptic connections which, in simulations, are each typically charaterized by at least one floating-point value.
This amounts to several terabytes of data -- an unrealistic memory requirement for a single desktop machine.
Large models are therefore typically simulated on distributed supercomputers which is costly and limits large-scale modelling to a few privileged research groups.
In this work, we describe extensions to GeNN -- our Graphical Processing Unit~(GPU) accelerated spiking neural network simulator -- that enable it to `procedurally' generate connectivity and synaptic weights `on the go' as spikes are triggered, instead of storing and retrieving them from memory.
We find that GPUs are well-suited to this approach because of their raw computational power which, due to memory bandwidth limitations, is often under-utilised when simulating spiking neural networks.
We demonstrate the value of our approach with a recent model of the Macaque visual cortex consisting of \num{4.13E6} neurons and \num{24.2E9} synapses.
Using our new method, it can be simulated on a single GPU -- a significant step forward in making large-scale brain modelling accessible to many more researchers.
Our results match those obtained on a supercomputer and the simulation runs up to \SI{35}{\percent} faster on a single high-end GPU than previously on over 1000 supercomputer nodes.
\end{abstract}
\maketitle
\section{Introduction}
The brain of a mouse has around \num{70E6} neurons, but this number is dwarfed by the \num{1E12} synapses which connect them~\citep{Herculano-Houzel2010}.
%While simulating synaptic plasticity -- the family of mechanisms believed to be %responsible for learning -- represents a further challenge, in a large-scale model, it is unlikely that learning would be enabled on \emph{all} synapses so efficiently simulating the remaining static synapses is a key challenge for large-scale brain simulation.\todo{has this been at all quantified in mice?}
In computer simulations of spiking neural networks, propagating spikes involves  adding the synaptic input from each spiking presynaptic neuron to the postsynaptic neurons.
%Because typical EPSP shaping functions are linear, they can then be subsequently applied to the `histogram' resulting from this process.~\todo{presumably someone first had this intuition so cite}.
The information describing which neurons are synaptically connected and with what weight is typically generated before a simulation is run and stored in large arrays. 
For large-scale brain models this creates high memory requirements, so that they can typically only be simulated on large distributed computer systems using software such as NEST~\citep{Gewaltig2007} or NEURON~\citep{carnevale2006neuron}.
By careful design, these simulators can keep the memory requirements for each node almost constant, even when a simulation is distributed across tens of thousands of nodes~\citep{Jordan2018}.
However, high performance computer~(HPC) systems are bulky, expensive and consume a lot of power and are hence typically shared resources, only accessible to a limited number of researchers and for time-limited investigations.

Neuromorphic systems~\citep{Frenkel2018,Furber2014,Merolla2014,Qiao2015,Schemmel2017} take inspiration from the brain and have been developed specifically for simulating large spiking neural networks more efficiently.
One particular relevant feature of the brain is that its memory elements -- the synapses -- are co-located with the computing elements -- the neurons.
In neuromorphic systems, this often translates to dedicating a large proportion of each chip to memory.
This allows some classes of spiking neural networks to be simulated very efficiently, but reducing the degree of connectivity to fit within the constraints of current neuromorphic systems inevitably changes the dynamics of brain simulations~\citep{VanAlbada2015}.
However, while such on-chip memory is fast, it can only be fabricated at relatively low density so that many of these systems economize -- either by reducing the maximum number of synapses per neuron to as few as \num{256} or by reducing the precision of the synaptic weights to \SI{6}{\bit}~\citep{Schemmel2017}, \SI{4}{\bit}~\citep{Frenkel2018} or even \SI{1}{\bit}~\citep{Merolla2014}.
Unlike most other neuromorphic systems, the SpiNNaker~\citep{Furber2014} neuromorphic supercomputer is fully programmable and combines large on-chip and external memories, distributed across the system, which enables real-time simulation of large-scale models~\citep{Rhodes2019}.
This is promising for the future but, due to its prototype nature, the availability of SpiNNaker hardware is limited and even moderately-sized simulations still require a physically large system (9 boards for a model with around \num{100E3} neurons and \num{300E6} synapses~\citep{Rhodes2019}).
%However, a physically large system is required for even moderately-sized simulations (9 boards for a simulation with around \num{100E3} neurons and \num{300E6} synapses~\citep{Rhodes2019})
%A next generation SpiNNaker system is currently under development~\citep{Mayr2019} and, by employing a newer fabrication technology (\SI{22}{\nano\meter} rather than \SI{130}{\nano\meter}), a single chip of the new system will offer equivalent performance to 48 of the current chips.
%Nonetheless, large-scale brain simulations will still require a large multi-chip system.

Modern GPUs have relatively little on-chip memory and, instead, dedicate the majority of their silicon area to arithmetic logic units.
GPUs use dedicated hardware to rapidly switch between tasks so that the latency of accessing external memory can be `hidden' behind computation, as long as there is sufficient computation to be performed.
For example, the memory latency of a typical modern GPU can be completely hidden if each CUDA core performs approximately 10 arithmetic operations per byte of data accessed from memory.
Unfortunately, propagating a spike in a spiking neural network simulation typically requires reading a synaptic weight ($\ge \SI{2}{\byte}$) and postsynaptic index ($\ge \SI{2}{\byte}$) from memory and then writing back the synaptic input ($\ge \SI{4}{\byte}$), while performing many fewer than the corresponding 80 instructions. 
This makes spike propagation highly memory bound.
Nonetheless, we have shown in previous work~\citep{Knight2018} that, as GPUs have significantly higher total memory bandwidth than even the fastest CPU, moderately sized models of around \num{100E3} neurons and \num{1E9} synapses can be simulated on a single GPU with competitive speed and energy consumption.
However, individual GPUs do not have enough memory to simulate larger brain models and, although small numbers of GPUs can be connected using the high-speed NVLink~\citep{NVIDIACorporation} interconnect, larger GPU clusters suffer from the same communication overheads as any other distributed HPC system.

In this work, we present a novel approach that uses the large amount of computational power available on a GPU to reduce both memory and memory bandwidth requirements and enable large-scale brain simulations on a single GPU workstation.

\section{Results}
In the following subsections, we first present two recent innovations in our GeNN simulator~\citep{Yavuz2016} which enable simulations of very large models on a GPU.
We then demonstrate the power of the new features by simulating a recent model of the Macaque visual cortex~\citep{Schmidt2018} with \num{4.13E6} neurons and \num{24.2E9} synapses.

\subsection{Procedural connectivity}
\label{sec:results/procedural}
The first crucial innovation that enables large-scale simulations on a GPU is what we call `procedural connectivity'.
In a brain simulation, neurons and synapses can be described by a variety of mathematical models but these are eventually all translated into time or event-driven update algorithms~\citep{Brette2007}.
Our GeNN simulator~\citep{Yavuz2016} uses code generation to convert neuron and synapse update algorithms -- described using `snippets' of C-like code -- into CUDA code for efficient GPU simulation.
Before a simulation can be run, its parameters, in particular the state variables and the synaptic connectivity, need to be initialised.
Traditionally, this is done by running initialisation algorithms on the main CPU prior to the simulation.
The results are stored in CPU memory, uploaded to GPU memory and then used during the simulation.
We have recently extended GeNN to use code generation from code snippets to also generate efficient, parallel code for model initialisation~\citep{Knight2018}.
Offloading initialisation to the GPU in this way made it around $20\times$ faster on a desktop PC~\citep{Knight2018}, demonstrating that initialisation algorithms are well-suited for GPU acceleration.
Here, we are going one step further.
We realised that, if each synaptic connection can be re-initialised in less than the 80 operations required to hide the latency incurred when fetching its \SI{8}{\byte} of state from memory, it could be faster and vastly more memory efficient to regenerate synaptic connections on demand rather than storing them in memory.
This is the concept of procedural connectivity.
It is applicable whenever synapses are static -- plastic synapses which change their weights during a simulation will have to be simulated in the traditional way.
Although a similar approach was used by Eugene Izhikevich for simulating an extremely large thalamo-cortical model with \num{1E11} neurons and \num{1E15} synapses on a modest PC cluster in 2005~\citep{Izhikevich2005} -- an incredible achievement -- it has not been subsequently applied to modern hardware.

We implemented procedural connectivity in GeNN by repurposing our previously developed parallel initialisation methods.
Instead of running them once for all synapses at the beginning of the simulation, we rerun the methods during the simulation to regenerate the outgoing synapses of each neuron that fires a spike and immediately use the identified connections and weights to run the post-synaptic code which calculates the effect of the spike onto other neurons.
This is possible because the outgoing synaptic connections from each neuron are typically largely independent from those of other neurons as we shall see from typical examples below.

\begin{figure}
    \begin{fullwidth}
        \centering
        \includegraphics{figures/performance_scaling}
        \captionsetup{width=6.69in}
        \caption{Simulation time performance scaling on a range of modern GPUs (colors). \textbf{A} The best performing approach at each scale on each GPU (indicated by the symbols). For the largest models, the procedural method is always best.
        \textbf{B} Raw performance of each approach on each GPU.
        Missing bars indicate insufficient memory to simulate.}
        \label{fig:performance_scaling}
    \end{fullwidth}
\end{figure}

In the absence of knowledge of the exact microscopic connectivity in the brain, there are a number of typical connectivity schemes that are used in brain models.
We will now discuss two typical examples and how they can be implemented efficiently on a GPU.
One very common connectivity scheme is the `fixed probability connector' for which each neuron in the presynaptic population is connected to each neuron in the postsynaptic population with fixed probability $P_{\text{conn}}$.
The postsynaptic targets of any presynaptic neuron can hence be sampled from a Bernoulli process with success probability $P_{\text{conn}}$.
One simple way of sampling from the Bernoulli process is to repeatedly draw samples from the uniform distribution $\text{Unif}[0, 1]$ and generate a synapse if the sample is less than $P_{\text{conn}}$.
However, for sparse connectivity ($P_{\text{conn}} \ll 1$), it is much more efficient to sample from the geometric distribution $\text{Geom}[P_{\text{conn}}]$ which governs the number of Bernoulli trials until the next success (i.e. a synapse).
The geometric distribution can be sampled in constant time by inverting the cumulative density function of the equivalent continuous distribution (the exponential distribution) to obtain $\frac{log(\text{Unif}[0, 1])}{log(1 - P_{\text{conn}})}$~\citep[p499]{DevroyeLuc2013}.
Note that, if we were to directly draw from the uniform distribution, the sampling for each potential synapse would be independent from any other potential synapse and all these operations could be performed in parallel.
However, for the more efficient `geometric sampling' employed here, the sampling for the post-synaptic targets of a presynaptic neuron must be done serially, but is still independent from the sampling for any other presynaptic neuron.

Another common scheme for defining connectivity is the `fixed total number connector' in which a fixed total number $N_{\text{syn}}$ of synapses is placed between randomly chosen partners from the pre- and postsynaptic populations.
In order to initialise this connectivity in parallel, the number of synapses that originate from each of the $N_{\text{pre}}$ presynaptic neurons must first be calculated by sampling from the multinomial distribution $\text{Mult}[N_{\text{syn}}, \{P_{n}, P_{n}, \ldots, P_{n}\}]$, where $P_{n} = \frac{1}{N_{\text{pre}}}$, on the host CPU up front because these numbers need to add to $N_{\text{syn}}$ and are hence not independent.
However, once the numbers of outgoing synapses are determined, the postsynaptic targets for a presynaptic neuron can be generated very efficiently in parallel by sampling from the discrete uniform distribution $\text{Unif}[0, N_{\text{post}}]$ where $N_{\text{post}}$ is the size of the postsynaptic population.
Note, that this can only be done because the targets of each presynaptic neuron are independent from those of any other presynaptic neuron.
Where synaptic weights and delays are not constant across synapses, but are described by some statistical distribution, they can also be sampled independently from each other and hence in parallel.
%
%Other typical connectivity schemes are described in the GeNN documentation\ref{}\todo{Uhm ... are they? Or should we gloss over this ...}
%'fixed number post' which is like fixed number total without the multinomial and 'fixed number pre' which we can't do easily :)

In order to use these parallel initialisation schemes for procedural connectivity, we require reproducible pseudorandom numbers that can be generated independently for each presynaptic neuron.
In principle this could be done with `convential' pseudorandom  number generators~(PRNGs), but each presynaptic neuron would need to maintain its own PRNG state which would lead to a significant memory overhead.
Instead, we use the `counter-based' Philox4$\times$32-10 PRNG~\citep{Salmon2011}.
Counter-based PRNGs are designed for parallel applications and essentially consist of a pseudo-random bijective function which takes a counter as an input (for Philox4$\times$32-10 a \SI{128}{\bit} number) and outputs a random number.
In constrast to convential PRNGs, this means that generating the $n^\text{th}$ random number in a stream has exactly the same cost as generating the `next' random number, allowing us to trivially divide up the random number stream between multiple parallel processes (in this case presynaptic neurons).
%\todo{do we need some more explanation of how you get from this to a network simulation?}

For an initial demonstration of the performance and scalability of procedural connectivity, we simulated a network initially designed to investigate signal propagation through cortical networks~\citep{Vogels2005} but subsequently widely used as a scalable benchmark~\citep{Brette2007}.
The network consists of $N$ integrate-and-fire neurons, partitioned into $\frac{4N}{5}$ excitatory and $\frac{N}{5}$ inhibitory neurons.
The two populations of neurons are connected to each other and with themselves with fixed probability $P_{\text{conn}}=\SI{10}{\percent}$.

We ran simulations of this network at scales ranging from \num{1E3} to \num{1E6} neurons (\num{100E3} to \num{100E9} synapses respectively) on a representative selection of NVIDIA GPU hardware:
Jetson Xavier NX, a low-power embedded system with \SI{8}{\giga\byte} (shared memory);
Geforce MX130, a laptop GPU with \SI{2}{\giga\byte};
Geforce GTX 1650, a low-end desktop GPU with \SI{4}{\giga\byte};
and Titan RTX, a high-end workstation GPU with \SI{24}{\giga\byte}.
Fig.~\ref{fig:performance_scaling} shows the duration of these simulations using our new procedural approach or using the standard approach of storing synaptic connections in memory employing two different data structures.
Both data structures are described in more detail in our previous work~\citep{Knight2018} but briefly, in the `sparse' data structure, a presynaptic neuron's postsynaptic targets are represented as an array of indices whereas, in the `bitfield' data structure, they are represented as a $N_{\text{post}}$ array of bits where a `1' at position $i$ indicates that there is a connection to postsynaptic neuron $i$ and a `0' that there is not.
None of our devices have enough memory to store the \num{100E9} synapses required for the largest scale using either data structure but, at the \num{100E3} neuron scale, the bitfield data structure allows the model to fit into the memory of several devices it otherwise would not.
However, not only is the new procedural approach the \emph{only} way of simulating models at the largest scales but, as Fig.~\ref{fig:performance_scaling} illustrates, even at smaller scales the performance of the procedural approach is competitive with and sometimes better than the standard approach.
All of the synapses in this model have the same synaptic weight meaning that they can be hard-coded into the procedural connectivity kernels.
However, if weights vary across synapses, the `bitfield' cannot be used and the memory constraints for the `sparse' representation become even more severe.
%
\subsection{Kernel merging}
NVIDIA GPUs are typically programmed in CUDA using a Single Instruction Multiple Thread~(SIMT) paradigm where programmers write `kernel' functions containing serial C-like code which is then executed in parallel across many virtual threads.
We call our second innovation `kernel merging' and it relates to the way these kernels are implemented.
While the procedural connectivity presented in the previous section allows simulating models which would otherwise not fit into the memory of a GPU, there are additional problems when using code generation for models with a large number of neuron and synapse populations.
GeNN and other SNN simulators which use code generation to generate all of their simulation code~\citep{Blundell2018} (as opposed to, for example NESTML~\citep{Plotnikov2016}, which uses code generation only to generate neuron simulation code) generate separate pieces of code for each population of neurons and synapses.
This allows optimizations such as hard-coding constant parameters and, although generating code for models with many populations will result in large code size, C++ CPU code can easily be divided between multiple modules and compiled in parallel, minimizing the effects on build time.
However, GPUs can only run a small number of kernels -- which are equivalent to modules in this context --  simultaneously (128 on the latest NVIDIA GPUs~\citep[p278]{NVIDIACorporation2019}).
Therefore, GeNN employs a ``Kernel Fusion'' approach~\citep{Wang2010} where multiple neuron populations are simulated within each kernel.
CUDA threads are grouped into `thread blocks' and to maximise performance, the code running on  threads within a thread block should not `diverge'.
To minimise this divergence, we add `padding' threads around each population of neurons so population and block boundaries are aligned.
Therefore, in the following pseudocode we simulating 3 populations of 100 neurons each in a single kernel and -- assuming a thread block size of 32 -- we pad each population to 128 threads:

\begin{lstlisting}
void updateNeurons() {
  if(thread < 128) {
    if(thread < 100) {
      // Update neuron population A
    }
  } else if(thread >= 128 && thread < 256) {
    if((thread - 128) < 100) {
      // Update neuron population B
    }
  } else if(thread >= 256 && thread < 384) {
    if((thread - 256) < 100) {
      // Update neuron population C
    }
  }
}
\end{lstlisting}
%
This approach works well for a small number of populations but, as Fig.~\ref{fig:merging_scaling}A illustrates, when we partition a model consisting of a single population of \num{1000000} LIF neurons, stimulated by a Gaussian current input into an increasingly large number of ever smaller populations, compilation time increases super-linearly and quickly becomes impractical.
Furthermore, the simulation also runs slower with a large number of populations (Fig.~\ref{fig:merging_scaling}B).
%
\begin{figure}
    \centering
    \includegraphics{figures/merging_scaling}
    \caption{Performance of a simulation of \num{1E6} LIF neurons driven by a Gaussian input current, partitioned into varying numbers~($N_{pop}$) of populations and running on a workstation equipped with a Titan RTX GPU.
    \textbf{A} Compilation time~($T_{comp}$) using GCC 7.5.0.
    \textbf{B} Simulation time~($T_{sim}$) for a \SI{1}{\second} simulation.
    \textbf{C} Memory throughput~($K_{mem}$) reported by NVIDIA Nsight compute profiler ``Speed of light'' metric.
    \textbf{D} Number of ``No instruction'' stalls reported by NVIDIA Nsight compute profiler~($N_{stall}$).}
    \label{fig:merging_scaling}
\end{figure}
%
Normally, we would expect this model to be memory bound as each thread in the model reads \SI{32}{\byte} of data (\SI{8}{\byte} of neuron state and \SI{24}{\byte} of RNG state) and, as discussed above, hiding the latency of these memory accesses would require approximately 320 arithmetic operations -- many more than required to sample an input current from the normal distribution and update a LIF neuron.
Fig.~\ref{fig:merging_scaling}C -- obtained using data from the NVIDIA Nsight compute profiler~\citep{NVIDIACorporation2020} -- shows that this is true for small numbers of populations.
In this case, the memory system is around \SI{90}{\percent} utilised.
However, when the model is partitioned into larger numbers of smaller populations, the memory is used less efficiently and the kernel becomes latency bound, i.e. neither memory \emph{nor} compute are used efficiently.
Investigating further, we found that this drop in performance was accompanied by an increasing number of ``No instruction'' stalls~(Fig.~\ref{fig:merging_scaling}D) which are events that prevent the GPU from doing any work during a clock cycle.
These particular events are likely to be caused by ``Excessively jumping across large blocks of assembly code''~\citep[p47]{NVIDIACorporation2020}, which makes sense as we are generating kernels with hundreds of thousands of lines of code.
%\todo{is more detail required here as to why?}

Several neural modelling tools including Brian2~\citep{Stimberg2019} provide modellers with tools to work with `slices' of neuron populations, allowing models to be defined with fewer populations.
However, if a model is defined by connecting these slices together, the resulting connectivity is the result of \emph{multiple} simple connection rules of the type discussed in the previous section, making it much more difficult to apply our procedural connectivity approach.
Furthermore, such an approach places the responsibility for structuring a model in such a way that it can be simulated efficiently onto the modellers, who often prefer to concentrate on the science and organise populations according to anatomy or physiology.

To address the issue of too many populations, we developed a new code generator for GeNN which applies a Single Program Multiple Data~(SPMD) approach and `merges' the model description, grouping together populations which can be simulated using the same generated code.
From this merged description, structures are generated to store the pointers to state variables and parameter values which are still allowed to differ between merged populations:
%
\begin{lstlisting}
struct NeuronUpdateGroup {
  unsigned int numNeurons;
  float* V;
};
\end{lstlisting}
%
An array of these structures is then declared for each merged population and each element is initialised with pointers to state variables and parameter values:
%
\begin{lstlisting}
NeuronUpdateGroup neuronUpdateGroup[3];
neuronUpdateGroup[0] = {100, VA};
neuronUpdateGroup[1] = {100, VB};
neuronUpdateGroup[2] = {100, VC};
\end{lstlisting}
%
where \lstinline{VA} is a pointer to the array containing the state variable `V' of populations `A' and so on.
In order for a thread to determine which neuron in which population it should simulate, we generate an additional data structure -- an array containing a cumulative sum of threads used for each population:
\begin{lstlisting}
unsigned int startThread[3] = {0, 128, 256};
\end{lstlisting}
Each thread performs a simple binary search within this array to find the index of the neuron and population it should simulate.
As Fig.~\ref{fig:merging_scaling} shows, this approach solves the observed issues with compilation time and simulation performance.
%
\begin{figure}
    \begin{fullwidth}
        \centering
        \includegraphics{figures/multi_area}
        \captionsetup{width=6.69in}
        \caption{Results of full-scale multi-area model simulation in ground resting states. 
        \textbf{A-F} Raster plots from GeNN simulation showing spiking activity of \SI{3}{\percent} of the neurons in area V1~(A,D), V2~(B,E), and FEF~(C,F).
        Blue: excitatory neurons, red: inhibitory neurons.
        \textbf{G-L} Spiking statistics for each population across all 32 areas simulated using GeNN and NEST shown as split violin plots.
        Solid lines: medians, Dashed lines: Interquartile range.
        \textbf{G,J} Population-averaged firing rates.
        \textbf{H,K} Average pairwise correlation coefficients of spiking activity. 
        \textbf{I,L} Irregularity measured by revised local variation LvR~\citep{Shinomoto2009} averaged across neurons.}
        \label{fig:multi_area}
    \end{fullwidth}
\end{figure}
%
\subsection{The multi-area model}
Due to lack of computing power and sufficiently detailed connectivity data, previous models of the cortex have either focussed on modelling individual local microcircuits at the level of individual cells~\citep{Izhikevich2008,Potjans2012} or modelling multiple connected areas at a higher level of abstraction~\citep{Cabral2014}.
However, recent data~\citep{Belitski2008} has shown that cortical activity has distinct features at both the global and local levels which can only be captured by modelling interconnected microcircuits at the level of individual cells.
The recent multi-area model~\citep{Schmidt2018a,Schmidt2018} is an example of such multi-scale modeling.
It uses scaled versions of a previous, 4 layer microcircuit model~\citep{Potjans2012} to implement \SI{1}{\milli\meter\squared} `patches' for 32 areas of the macaque visual cortex.
The patches are connected together according to inter-area axon tracing data from the CoCoMac~\citep{Bakker2012} database, further refined using additional anatomical data~\citep{Markov2014} and heuristics~\citep{Ercsey-Ravasz2013} to obtain estimates for the number of synapses between areas.
The synapses are distributed between populations in the source and target area using layer-specific tracing data~\citep{Markov2014b} and cell-type-specific dendritic densities~\citep{Binzegger2004}.
Individual populations are connected by the fixed number connectors described above.
For a full description of the multi-area model please see~\citet{Schmidt2018a,Schmidt2018}.
In 2018, this model was simulated using NEST~\citep{Gewaltig2007} on one rack of an IBM Blue Gene/Q supercomputer~(a \SI{2}{\metre} high enclosure containing \num{1024} compute nodes, weighing over \SI{2}{\tonne} and requiring around \SI{80}{\kilo\watt} of power).
On this system, initialization of the model took around \SI{5}{\minute} and simulating \SI{1}{\second} of biological time took approximately \SI{12}{\minute}~\citep{Schmidt2018}.

The multi-area model consists of \num{4.13E6} neurons in \num{254} populations and \num{24.2E9} synapses in \num{64516} populations.
Without kernel merging, it would therefore be unlikely that the model would compile or simulate at a workable speed using GeNN.
Additionally, unlike the model we benchmarked previously, each synapse in this model has an independant weight and synaptic delay sampled from a normal distribution so the bitfield data structure cannot be used.
Even if we assume that \SI{16}{\bit} floating-point would provide sufficient weight precision, that delays could be expressed as \SI{8}{\bit} integers and that neuron populations are all small enough to be indexed using \SI{16}{\bit} indices, our sparse data structure would still require \SI{5}{\byte} per synapse, such that the complete synaptic data would need over \SI{100}{\giga\byte} of GPU memory.
While a cluster of GPUs connected using NVLink could be built with this much memory, it is more than any single GPU has available.
However, using procedural connectivity, we are able to simulate this model on a single workstation with a Titan RTX GPU.

In order to validate our GeNN simulations, we ran a \SI{10.5}{\second} simulation of the multi-area model in a `ground state' where inter-area connections have the same strength as intra-area connections and a \SI{100.5}{\second} simulation in a `resting state' where inter-area connections are $1.9\times$ stronger.
Initialization of our model took \SI{6}{\minute} (\SI{3}{\minute} of which was spent generating and compiling code) and simulation of each biological second took \SI{7.7}{\minute} in the ground state and \SI{8.4}{\minute} in the resting state -- \SI{35}{\percent} and \SI{30}{\percent} less than the supercomputer simulation respectively.
Fig.~\ref{fig:multi_area}A-C shows some example spike rasters from three of the modelled areas, illustrating the asynchronous irregular nature of the model's ground state whereas Fig.~\ref{fig:multi_area}D-F illustrate the characteristic irregular activity and population bursts of the same areas in the resting state.
Next, we calculated the per-layer distributions of rates, spike-train irregularity and cross-correlation coefficients across all areas (disregarding the first \SI{500}{\milli\second} of simulation) and compared them to the same measures obtained from spike trains generated by the supercomputer simulations.
We calculated irregularity using the revised local variation LvR~\citep{Shinomoto2009}, averaged over a subsample of \num{2000} neurons and cross-correlation from spike histograms with \SI{1}{\milli\second} bins, calculated from a subset of \num{2000} non-silent neurons.
The violin plots in Fig.~\ref{fig:multi_area}G-L show the comparison of the distributions of values obtained from the NEST and GeNN simulations in both states -- which are essentially identical.
%
\section{Discussion}
In this work we have presented a novel approach for large-scale brain simulation on GPU devices which entirely removes the need to store connectivity data in memory.
We have shown that this approach allows us to simulate a cortical model with \num{4.13E6} neurons and \num{24.2E9} synapses~\citep{Schmidt2018a,Schmidt2018} on a single modern GPU.
While this represents a significant step forward in terms of making large-scale brain modelling tools accesible to a large community of brain researchers, this model still has around $20\times$ fewer neurons and $40\times$ fewer synapses than the brain of even a small mammal such as a mouse~\citep{Herculano-Houzel2010}.
Our implementation of the multi-area model requires a little over \SI{12}{\giga\byte} of GPU memory, with the majority~(\SI{8.5}{\giga\byte}) being used for the circular dendritic delay buffers~(see~\citet{Knight2018}).
These are a per-neuron (rather than per-synapse) data structure but, because the inter-area connections in the model have delays of up to 500 \SI{0.1}{\milli\second} simulation timesteps, the delay buffers become quite large.
However, for models without large delays such as the balanced random network simulated in section~\ref{sec:results/procedural}, only around \SI{20}{\byte} is required per neuron meaning that, theoretically, over \num{1E9} neurons could be simulated on a \SI{24}{\giga\byte} GPU.

One important aspect of large-scale brain simulations not addressed in this work is synaptic plasticity and its role in learning.
As discussed by~\citet{Knight2018}, GeNN supports a wide variety of synaptic plasticity rules.
In order to modify synaptic weights, they need to be stored in memory rather than generated procedurally.
However, connectivity could still be generated procedurally, potentially halving the memory requirements of models with synaptic plasticity.
This would be sufficient for synaptic plasticity rules that only require access to presynaptic spikes and postsynaptic neuron states~\citep{Brader2007,Clopath2010c} but, for many Spike-Timing-Dependent Plasticity~(STDP) rules, access to \emph{postsynaptic} spikes is also required.
GeNN supports such rules by automatically generating a lookup table structure (see~\citet{Knight2018}).
While this process could be adapted to generate a lookup table from procedural connectivity, this would further erode memory savings.
However, typically not all synapses in a simulation are plastic and those that are not could be simulated fully procedurally.

In this work, we have discussed the idea of procedural connectivity in the context of GPU hardware but, we believe that there is also potential for developing new types of neuromorphic hardware built from the ground up for procedural connectivity.
Key components such as the random number generator could be implemented directly in hardware leading to truly game-changing compute time improvements.
%
\section{Methods}
\begin{table}
  \centering
  \begin{tabular}{r S S S}
    \toprule
        {Parameter}                             & {Procedural connectivity} & {Merging}         & {Multi-area} \\
                                                & {benchmark}               & {benchmark}       & {model} \\
    \midrule
        $\tau_{\text{m}}$ [\si{\milli\second}]         & 20                        & 20                & 2 \\
        $V_{\text{rest}}$ [\si{\milli\volt}]           & -60.0                     & -70.0             & -65 \\
       
        $V_{\text{th}}$ [\si{\milli\volt}]         & -50.0                     & -51.0             & -50 \\
        $R_{\text{m}}$ [\si{\mega\ohm}]                  & 20                        & 20                & 40 \\
        $\tau_{\text{syn}}$ [\si{\milli\second}]       & {$5/10$\textsuperscript{1}} & {---}             & 0.5\\
        $\tau_{\text{ref}}$ [\si{\milli\second}]       & 5                         & 2                 & 2 \\
        $I_{\text{ext}_j}$ [\si{\nano\ampere}]           & 0.55                      & 1.0 \pm 0.25   & {Poisson\textsuperscript{2}}\\
        $w_{ij}$ [\si{\nano\ampere}]            & {$\frac{3.2}{N}/\frac{40.8}{N}$\textsuperscript{1}}           & {---}            & {Various\textsuperscript{2}}\\
    \bottomrule
  \end{tabular}
  \caption{Model parameters. \\
  \textsuperscript{1}Excitatory/Inhibitory. \\
  \textsuperscript{2}Please refer to \citet[Table 1,2]{Schmidt2018}}
  \label{tab:parameters}
\end{table}
In all experiments presented in this work, neurons are modelled as leaky integrate-and-fire~(LIF) units with the parameters listed in Table~\ref{tab:parameters}.
The membrane voltage $V_{i}$ of neuron $i$ is modelled as
%
\begin{align}
    \tau_{\text{m}} \frac{dV_{i}}{dt} = & (V_{i} - V_{\text{rest}}) + R_{\text{m}}(I_{\text{syn}_{i}} + I_{\text{ext}_{i}}), \label{eq:lif_neuron}
\end{align}
%
where $\tau_{\text{m}}$ and $R_{\text{m}}$ represent the time constant and resistance of the neuron's cell membrane, $V_{\text{rest}}$ defines the resting potential, $I_{\text{syn}_{i}}$ represents the synaptic input current and $I_{\text{ext}_i}$ represents an external input current.
When the membrane voltage crosses a threshold~$V_{\text{th}}$ a spike is emitted, the membrane voltage is reset to $V_{\text{rest}}$ and updating of $V$ is suspended for a refractory period $\tau_{\text{ref}}$.
In the models where there are synaptic connections, pre-synaptic spikes lead to exponentially-decaying input currents $I_{\text{syn}_{i}}$
%
\begin{align}
    \tau_{\text{syn}} \frac{dI_{\text{syn}_{i}}}{dt} = & -I_{\text{syn}_{i}} + \sum_{i=0}^{n} w_{ij} \sum_{t_{j}}  \delta(t - t_{j}),\label{eq:exp_neuron_input_current}
\end{align}
%
where $\tau_{\text{syn}}$ represents the decay time constant and $t_{j}$ are the arrival times of incoming spikes from $n$ presynaptic neurons.
The continuous terms of the Eq.~\ref{eq:lif_neuron}~and~\ref{eq:exp_neuron_input_current} are separately solved algebraically so that the input currents $I_{\text{syn}_{i}}$ and $I_{\text{ext}_{i}}$ are treated as constants throughout each simulation timestep.

\section{Acknowledgements}
We would like to thank Jari Pronold, Sacha van Albada, Agnes Korcsak-Gorzo and Maximilian Schmidt for their help with the multi-area model data; and Dan Goodman and Mantas Mikaitis for their feedback on the manuscript.
This work was funded by the EPSRC (Brains on Board project, grant number EP/P006094/1).

\section{Author contributions}
J.K. and T.N. wrote the paper.
T.N. is the original developer of GeNN.
J.K. is currently the primary GeNN developer and was responsible for extending the code generation approach to the procedural simulation of synaptic connectivity.
J.K. performed the experiments and the analysis of the results that are presented in this work.

\section{Data availability}
The raw data used to produce Fig.~\ref{fig:performance_scaling} and Fig.~\ref{fig:merging_scaling}; and the pre-processed data used to produce Fig.~\ref{fig:multi_area} are available at \url{https://github.com/BrainsOnBoard/procedural_paper}.
The raw spiking data from the multi-area model simulations is available upon request.

\section{Code availability}
All experiments were carried out using the GeNN 4.3.0, available at \url{https://github.com/genn-team/genn/releases/tag/4.3.0}.
A GeNN port of the multi-area model is available at \url{https://github.com/neworderofjamie/multi-area-model}.
The models used to produce Fig.~\ref{fig:performance_scaling} and Fig.~\ref{fig:merging_scaling} are all available at \url{https://github.com/BrainsOnBoard/procedural_paper}.

% Bibliography
\bibliographystyle{apalike}
\bibliography{procedural}

\end{document}
